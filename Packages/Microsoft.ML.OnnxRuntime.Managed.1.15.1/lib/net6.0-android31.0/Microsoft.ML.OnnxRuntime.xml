<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.ML.OnnxRuntime</name>
    </assembly>
    <members>
        <member name="T:Microsoft.ML.OnnxRuntime.IDisposableReadOnlyCollection`1">
            <summary>
            Return immutable collection of results
            </summary>
            <typeparam name="T"></typeparam>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue">
            <summary>
            This class serves as a container for model run output values including
            tensors, sequences of tensors, sequences and maps.
            The class must be disposed of.
            It disposes of _ortValueHolder that owns the underlying Ort output value and
            anything else that would need to be disposed by the instance of the class.
            Use factory method CreateFromOrtValue to obtain an instance of the class.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,Microsoft.ML.OnnxRuntime.IOrtValueOwner)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.OnnxValueType,Microsoft.ML.OnnxRuntime.IOrtValueOwner)">
            <summary>
            Ctor for non-tensor values
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="onnxValueType"></param>
            <param name="ortValueHolder"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.MapHelper,Microsoft.ML.OnnxRuntime.IOrtValueOwner)">
            <summary>
            Construct an instance that would contain a map in a form of a Dictionary
            Currently a limited number of primitive types are supported as map keys and values.
            So this is not a full implementation of the map type.
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="mapHelper"></param>
            <param name="ortValueHolder"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.ElementType">
            <summary>
            Only valid if ValueType is Tensor
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.InputToOrtValue(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Overrides the base class method. Since the instance already owns underlying OrtValue handle,
            it returns an instance of OrtValue that does not own the raw handle
            that to the output onnxValue. With respect to pinnedMemoryHandle, it has no operation
            to do, as this class maintains a native buffer via _ortValueHolder and the memory will be
            disposed by it. This is the case when we are dealing with an OrtValue that is backed by native memory
            and not by pinned managed memory.
            
            This class is generally used for outputs to be created on top of the output OrtValue,
            but the interface (derived from NamedOnnxValue) allows it to be passed as input and one of the test
            cases does it. Unless we deprecate and re-do the interface, we must support it.
            </summary>
            <param name="pinnedMemoryHandle">always set to null</param>
            <returns>An instance of OrtValue that does not own underlying memory</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.OutputToOrtValue(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Generally, this class is created on top of the values that are returned by the model run.
            So, this method is not expected to be called. However, if it is called (an instance fed as output),
            it will return the OrtValue that was previously created, since the caller must understand what they are doing.
            </summary>
            <param name="metadata"></param>
            <param name="memoryOwner"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeTensor(System.String,Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            Creates an instance of DisposableNamedOnnxValue and takes ownership of ortValueElement
            on success.
            </summary>
            <param name="name">name of the value</param>
            <param name="ortValue">underlying OrtValue</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeTensor``1(System.String,Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            This method creates an instance of DisposableNamedOnnxValue that has possession of ortValueElement
            native memory Tensor and returns it to the caller. The original ortValueElement argument looses
            ownership of the native ortValueElement handle, however, the caller is still responsible for disposing them
            on exception. Disposing of OrtValue that has no ownership is a no-op and fine.
            </summary>
            <typeparam name="T">data type</typeparam>
            <param name="name">name of the output</param>
            <param name="ortValue">native tensor</param>
            <returns>DisposableNamedOnnxValue instance</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeSequence(System.String,Microsoft.ML.OnnxRuntime.OrtValue,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            This method will create an instance of DisposableNamedOnnxValue that will own ortSequenceValue
            an all disposable native objects that are elements of the sequence
            </summary>
            <param name="name"></param>
            <param name="ortValueSequence">ortValueElement that has native sequence</param>
            <param name="allocator"> used allocator</param>
            <returns>DisposableNamedOnnxValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeMap(System.String,Microsoft.ML.OnnxRuntime.OrtValue,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            Will extract keys and values from the map and create a DisposableNamedOnnxValue from it
            </summary>
            <param name="name">name of the output</param>
            <param name="ortValueMap">ortValue that represents a map. 
            This function does not take ownership of the map as it we copy all keys an values into a dictionary. We let the caller dispose of it</param>
            <param name="allocator"></param>
            <returns>DisposableNamedOnnxValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.FromNativeMapElements``2(System.String,Microsoft.ML.OnnxRuntime.OrtValue,Microsoft.ML.OnnxRuntime.OrtValue,Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            This method maps keys and values of the map and copies them into a Dictionary
            and returns as an instance of DisposableNamedOnnxValue that does not own or dispose
            any onnx/ortValueElement. The method takes possession of ortValueTensorKeys and ortValueTensorValues
            and disposes of them. The original ortValueElement looses ownership of the Tensor. The caller is still responsible
            for disposing these arguments. Disposing ortValueElement that does not have ownership is a no-op, however, either
            of the arguments may still need to be disposed on exception.
            </summary>
            <typeparam name="K">key type</typeparam>
            <typeparam name="V">value type</typeparam>
            <param name="name">name of the output parameter</param>
            <param name="ortValueTensorKeys">tensor with map keys.</param>
            <param name="nativeOnnxValueValues">tensor with map values</param>
            <returns>instance of DisposableNamedOnnxValue with Dictionary</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked by Dispose()</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.DisposableNamedOnnxValue.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ErrorCode">
            <summary>
            Enum conresponding to native onnxruntime error codes. Must be in sync with the native API
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException">
            <summary>
            The Exception that is thrown for errors related ton OnnxRuntime
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue">
            <summary>
            Represents an OrtValue with its underlying buffer pinned
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.CreateFromTensor``1(Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0})">
            <summary>
            Creates a <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> object from the tensor and pins its underlying buffer.
            </summary>
            <typeparam name="T"></typeparam>
            <param name="value"></param>
            <returns>a disposable instance of FixedBufferOnnxValue</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.CreateFromMemory``1(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,System.Memory{``0},Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],System.Int64)" -->
        <member name="M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked from Dispose()</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.InferenceSession">
            <summary>
            Represents an Inference Session on an ONNX Model.
            This is a IDisposable class and it must be disposed of
            using either a explicit call to Dispose() method or
            a pattern of using() block. If this is a member of another
            class that class must also become IDisposable and it must
            dispose of InferfenceSession in its Dispose() method.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._nativeHandle">
            <summary>
            A pointer to a underlying native instance of OrtSession
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._inputMetadata">
            <summary>
            Dictionary that represents input metadata
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._inputNames">
            <summary>
            Ordered list of input names
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._outputMetadata">
            <summary>
            Dictionary that represent output metadata
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._outputNames">
            <summary>
            Ordered list of output names
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._overridableInitializerMetadata">
            <summary>
            Dictionary that represents overridableInitializers metadata
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.InferenceSession._namesMemoryPtrs">
            <summary>
            This list holds Utf-8 converted input/output names allocated from a native heap
            and as such do not require pinning. It must be disposed of (freed).
            
            Introduced to reduce the GC burden as the names are used in every Run() call.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String)">
            <summary>
            Constructs an InferenceSession from a model file
            </summary>
            <param name="modelPath"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String,Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model file and it will use 
            the provided pre-packed weights container to store and share pre-packed buffers 
            of shared initializers across sessions if any.
            </summary>
            <param name="modelPath">Model path</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String,Microsoft.ML.OnnxRuntime.SessionOptions)">
            <summary>
            Constructs an InferenceSession from a model file, with some additional session options
            </summary>
            <param name="modelPath"></param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.String,Microsoft.ML.OnnxRuntime.SessionOptions,Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model file, with some additional session options
            and it will use the provided pre-packed weights container to store and share pre-packed buffers 
            of shared initializers across sessions if any.
            </summary>
            <param name="modelPath">Model path</param>
            <param name="options">Session options</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[])">
            <summary>
            Constructs an InferenceSession from a model data in byte array
            </summary>
            <param name="model"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model data (in byte array) and it will use 
            the provided pre-packed weights container to store and share pre-packed buffers 
            of shared initializers across sessions if any.
            </summary>
            <param name="model">Model as byte array</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.SessionOptions)">
            <summary>
            Constructs an InferenceSession from a model data in byte array, with some additional session options
            </summary>
            <param name="model"></param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.SessionOptions,Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer)">
            <summary>
            Constructs an InferenceSession from a model data (in byte array) with some additional
            session options and it will use the provided pre-packed weights container to store
            and share pre-packed buffers of shared initializers across sessions if any.
            </summary>
            <param name="model">Model as byte array</param>
            <param name="options">Session Options</param>
            <param name="prepackedWeightsContainer">Instance of PrepackedWeightsContainer. 
            Lifetime of 'prepackedWeightsContainer' must be
            managed by the user and it must outlive any sessions reliant on it</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.InputMetadata">
            <summary>
            Meta data regarding the input nodes, keyed by input names
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.InputNames">
            <summary>
            Ordered list of input names that can be accessed by index;
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.OutputMetadata">
            <summary>
            Metadata regarding the output nodes, keyed by output names
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.OutputNames">
            <summary>
            Ordered list of output names that can be accessed by index.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.OverridableInitializerMetadata">
            <summary>
            Metadata regarding the overridable initializers, keyed by node names
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs, and fetches all the outputs.
            </summary>
            <param name="inputs">specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            Runs the loaded model for the given inputs, and fetches the outputs specified in <paramref name="outputNames"/>.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs, and fetches the specified outputs in <paramref name="outputNames"/>. Uses the given RunOptions for this run.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <param name="options"></param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs, and fetches all the outputs.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            Runs the loaded model for the given inputs, and fetches the outputs specified in <paramref name="outputNames"/>.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs, and fetches the specified outputs in <paramref name="outputNames"/>. Uses the given RunOptions for this run.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names to fetch.</param>
            <param name="options"></param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs and outputs.
            
            Outputs need to be created with correct type and dimension to accept the fetched data.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
            Outputs need to be created with correct type and dimension to accept the fetched data.
            </summary>
            <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs and outputs.
            
            Outputs need to be created with correct type and dimension to receive the fetched data.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
             <summary>
             
             Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
             Outputs need to be created with correct type and dimension to receive the fetched data.
             </summary>
             <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
             <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
             <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs and outputs.
            
            Outputs need to be created with correct type and dimension to receive the fetched data.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
            Outputs need to be created with correct type and dimension to receive the fetched data.
            </summary>
            <param name="inputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the input values.</param>
            <param name="outputNames">Specify a collection of string that indicates the output names. Should match <paramref name="outputValues"/>.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
            <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue})">
             <summary>
            
             Runs the loaded model for the given inputs and outputs.
            
             Outputs need to be created with correct type and dimension to receive the fetched data.
             </summary>
             <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
             <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Run(System.Collections.Generic.IReadOnlyCollection{System.String},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},Microsoft.ML.OnnxRuntime.RunOptions)">
             <summary>
             
             Runs the loaded model for the given inputs and outputs. Uses the given RunOptions for this run.
            
             Outputs need to be created with correct type and dimension to receive the fetched data.
             </summary>
             <param name="inputNames">Specify a collection of string that indicates the input names. Should match <paramref name="inputValues"/>.</param>
             <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
             <param name="outputs">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.NamedOnnxValue"/> that indicates the output values.</param>
             <param name="options"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.CreateIoBinding">
            <summary>
            Create OrtIoBinding instance to bind pre-allocated buffers
            to input/output
            </summary>
            <returns>A new instance of OrtIoBinding</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.RunWithBinding(Microsoft.ML.OnnxRuntime.RunOptions,Microsoft.ML.OnnxRuntime.OrtIoBinding)">
            <summary>
            This method runs inference on the OrtIoBinding instance
            The method does not return anything. This is a lightweight version of 
            RunWithBindingAndNames(). When you bind pre-allocated buffers to the output values
            you may not want to fetch the outputs since you already have access to them so you can spare
            the expense of fetching them and pairing with names.
            You can still fetch the outputs by calling OrtIOBinding.GetOutputValues()
            </summary>
            <param name="runOptions">runOptions</param>
            <param name="ioBinding">ioBinding instance to use</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.RunWithBindingAndNames(Microsoft.ML.OnnxRuntime.RunOptions,Microsoft.ML.OnnxRuntime.OrtIoBinding,System.String[])">
            <summary>
             This method return a collection of DisposableNamedOnnxValue as in other interfaces
             Query names from OrtIoBinding object and pair then with the array of OrtValues returned
            from OrtIoBinding.GetOutputValues()
            
            </summary>
            <param name="runOptions">RunOptions</param>
            <param name="ioBinding">OrtIoBinding instance with bindings</param>
            <param name="names">optional parameter. If you already know the names of the outputs you can save a native
            call to retrieve output names. They will be paired with the returned OrtValues and combined into DisposbleNamedOnnxValues.
            Otherwise, the method will retrieve output names from the OrtIoBinding instance.
            It is an error if you supply a different number of names than the returned outputs</param>
            <returns>A disposable collection of DisposableNamedOnnxValue that encapsulate output OrtValues</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.EndProfiling">
            <summary>
            Ends profiling for the session.
            </summary>
            <returns> Returns the profile file name.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.LookupInputMetadata(System.String)">
            <summary>
            Checks if the name is a known input or overridable initializer name
            and if so, returns metadata for it.
            metadata
            </summary>
            <param name="nodeName"></param>
            <returns>NodeMetadata for the nodeName</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.LookupOutputMetadata(System.String)">
            <summary>
            Checks if the nodeName is a known output name and if so returns metadata for it.
            </summary>
            <param name="nodeName"></param>
            <returns></returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.ExtractOrtValueForInput(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Fetches/creates OrtValue for the content of the input
            </summary>
            <param name="input"></param>
            <param name="metadata"></param>
            <param name="memOwner"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.ExtractOrtValueForOutput(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Fetches/Creates OrtValue for output
            </summary>
            <param name="output"></param>
            <param name="metadata"></param>
            <param name="memOwner"></param>
            <returns>May return null if the onnx value type does not support pre-creation of output OrtValues</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.LookupUtf8Names``1(System.Collections.Generic.IReadOnlyCollection{``0},Microsoft.ML.OnnxRuntime.InferenceSession.NameExtractor{``0},Microsoft.ML.OnnxRuntime.InferenceSession.MetadataLookup)">
            <summary>
            Run helper
            </summary>
            <param name="values">names to convert to zero terminated utf8 and pin</param>
            <param name="nameExtractor">extractor functor that helps extracting names from inputs</param>
            <param name="metaDict">inputs/outputs metadata</param>
            <param name="cleanupList">list to add pinned memory to for later disposal</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.GetOrtValuesHandles(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.NamedOnnxValue},Microsoft.ML.OnnxRuntime.InferenceSession.MetadataLookup,Microsoft.ML.OnnxRuntime.InferenceSession.OrtValueExtractor,Microsoft.ML.OnnxRuntime.DisposableList{System.IDisposable})">
            <summary>
            This function obtains ortValues for NamedOnnxValue.
            The problem with NamedOnnxValue is that it is not disposable and can not contain any disposable items.
            so calling InputToOrtValue creates a new instance of OrtValue that needs to be disposed.
            The deriving object DisposableNamedValue actually contains and owns OrtValue and it returns
            it.
            </summary>
            <param name="values">a collection of NamedOnnxValues</param>
            <param name="metaLookup">Metadata lookup function (input/initializers/output)</param>
            <param name="cleanupList">list to cleanup in an exception safe manner</param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.ModelMetadata">
            <summary>
            This property queries model metadata, constructs
            an instance of ModelMetadata and caches it
            </summary>
            <returns>Instance of ModelMetdata</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.ProfilingStartTimeNs">
            <summary>
            Return the nanoseconds of profiling's start time
            On some platforms, this timer may not be as precise as nanoseconds
            For instance, on Windows and MacOS, the precision will be ~100ns
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.InitWithSessionHandle(System.IntPtr)">
            <summary>
            Initializes the session object with a native session handle
            </summary>
            <param name="session">Value of a native session object</param>
            <param name="options">Session options</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.InferenceSession.Handle">
            <summary>
            Other classes access
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Finalize">
            <summary>
            Finalizer. to cleanup session in case it runs
            and the user forgets to Dispose() of the session
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked from Dispose() method</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.InferenceSession.DisposeImpl(System.Boolean)">
            <summary>
            This function is also used on failure in the constructor
            </summary>
            <param name="disposing"></param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.TensorTypeAndShape">
            <summary>
            Represents tensor element type and its shapes
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.ElementDataType">
            <summary>
            Tensor Element type
            </summary>
            <value>TensorElementType enum</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.Dimensions">
            <summary>
            Shape
            </summary>
            <value>Array of dimensions</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.SymbolicDimensions">
            <summary>
            Symbolic dimensions
            </summary>
            <value>Array of symbolic dimensions if present.</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TensorTypeAndShape.ElementTypeInfo">
            <summary>
            Tensor element metadata
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.SequenceMetadata">
            <summary>
            Represents sequnce metdata
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SequenceMetadata.#ctor(Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            __ctor
            </summary>
            <param name="elementData"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SequenceMetadata.ElementMeta">
            <summary>
            Element Metatada, recursive definition with a Tensor being a base case
            may contain maps, tensors and other sequences
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OptionalMetadata">
            <summary>
            The class contains metadata for an optional input/output
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OptionalMetadata.#ctor(Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            __ctor
            </summary>
            <param name="elementData"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OptionalMetadata.ElementMeta">
            <summary>
            Element Metatada, recursive definition with a Tensor being a base case
            may contain maps, tensors and sequences
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MapMetadata">
            <summary>
            Represents Map MetaData.
            Key is always a tensor denoted by an element type
            with value type being a recursive structure that may
            contain other maps, sequences or tensors.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.MapMetadata.KeyDataType">
            <summary>
            Key tensor data type
            </summary>
            <value>A value of TensorElementType enum</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.MapMetadata.ValueMetadata">
            <summary>
            Value metadata
            </summary>
            /// <value>Instance of Nodemetadata for the value of the map</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NodeMetadata">
            <summary>
            Resembles type and shape information of session-graph nodes, used for communicating the shape/type of input/output nodes
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.OnnxValueType,Microsoft.ML.OnnxRuntime.TensorTypeAndShape)">
            <summary>
            Constructs NodeMetadata for tensor
            </summary>
            <param name="onnxValueType">either ONNX_TYPE_TENSOR or ONNX_TYPE_SPARSETENSOR</param>
            <param name="typeAndShape">Tensor type and shape information</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.MapMetadata)">
            <summary>
            __ctor for map metadata
            </summary>
            <param name="mapMetadata"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.SequenceMetadata)">
            <summary>
            __ctor for sequence metadata
            </summary>
            <param name="sequenceMetadata"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.#ctor(Microsoft.ML.OnnxRuntime.OptionalMetadata)">
            <summary>
            __ctor
            </summary>
            <param name="optMetadata"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.AsMapMetadata">
            <summary>
            Retrieves MapMetadata, valid only if this node represents a Map.
            </summary>
            <returns></returns>
            <exception cref="T:System.InvalidOperationException">when the instance does not contain map metadata</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.AsSequenceMetadata">
            <summary>
            Retrieves SequenceMetadata, valid only if this node represents a Sequence
            </summary>
            <returns></returns>
            <exception cref="T:System.InvalidOperationException">when the instance does not contain sequence metadata</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NodeMetadata.AsOptionalMetadata">
            <summary>
            Retrieves Optional type metadata, valid if this node is optional
            Optional metadata is nothing more than just a container for all the usual
            element types.
            </summary>
            <returns></returns>
            <exception cref="T:System.InvalidOperationException"></exception>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.OnnxValueType">
            <summary>
            Type value of the node
            </summary>
            <value>A value of OnnxValueType enum</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.ZeroTerminatedName">
            <summary>
            Node name in the natively allocated memory.
            
            Present only on the top-level instance
            metadata dictionary entries.
            
            Avoids repeated conversion and pinning
            
            This memory chunk is owned and freed by the InferenceSession
            object.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.Dimensions">
            <summary>
            Tensor shape valid only if this is a Tensor.
            Preserved for API compatibility
            </summary>
            <value>Array of dimensions</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.SymbolicDimensions">
            <summary>
            Symbolic dimensions valid only if this is a Tensor.
            Preserved for API compatibility
            </summary>
            <value>Array of symbolic dimensions if present.</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.ElementType">
            <summary>
            .NET type that corresponds to the primitive Tensor data type.
            Valid only if this is a Tensor.
            </summary>
            <value>System.Type</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.ElementDataType">
            <summary>
            Tensor Element Type. Valid if tensor
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.IsString">
            <summary>
            Convinience method to check for string
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NodeMetadata.IsTensor">
            <summary>
            Whether it is a Tensor
            </summary>
            <value>currently always returns true</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ModelMetadata">
            <summary>
            A class that queries and caches model metadata and exposes
            it as properties
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.ProducerName">
            <summary>
            Producer name string
            </summary>
            <value>producer name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.GraphName">
            <summary>
            Graph name for this model
            </summary>
            <value>graph name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.Domain">
            <summary>
            Domain for this model
            </summary>
            <value>domain name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.Description">
            <summary>
            Unstructured model description
            </summary>
            <value>description string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.GraphDescription">
            <summary>
            Unstructured graph description
            </summary>
            <value>description string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ModelMetadata.Version">
            <summary>
            Version number
            </summary>
            <value>long version integer</value>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.ModelMetadata.CustomMetadataMap" -->
        <member name="T:Microsoft.ML.OnnxRuntime.ManagedTypeProjection">
            <summary>
            The class helps to feed the NamedOnnxValue as inference input.
            It projects managed classes to OrtValues so they can be consumed
            by the native onnxruntime library. if possible, it will avoid copying data.
            The NamedOnnxValue can be a tensor, sequence or map.
            For recursive structures, create nested NamedOnnxValue instances.
            For example, a sequence instance would contain a list of NamedOnnxValue instances
            that in turn may represent tensors or other ONNX values.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.Value">
            <summary>
            Provides access to non-owning instance of OrtValue
            </summary>
            <value>Provides access to the OrtValue to be used as input</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.#ctor(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata)">
            <summary>
            Constructor to create an input OrtValue projection from managed data
            </summary>
            <param name="namedOnnxValue"></param>
            <param name="metadata"></param>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateDispatchProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,Microsoft.ML.OnnxRuntime.DisposableList{System.IDisposable})">
            <summary>
            Dispatches the creation of the projection
            </summary>
            <param name="namedOnnxValue"></param>
            <param name="metadata"></param>
            <param name="disposables"></param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateSequenceProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,Microsoft.ML.OnnxRuntime.DisposableList{System.IDisposable})" -->
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateMapProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,Microsoft.ML.OnnxRuntime.DisposableList{System.IDisposable})">
            <summary>
            Creates map projection. Since we support only primitive types in maps
            we map two tensors (keys and values)
            </summary>
            <param name="node"></param>
            <param name="elementMeta"></param>
            <param name="disposables"></param>
            <returns>OrtValue</returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.CreateTensorProjection(Microsoft.ML.OnnxRuntime.NamedOnnxValue,Microsoft.ML.OnnxRuntime.NodeMetadata,Microsoft.ML.OnnxRuntime.DisposableList{System.IDisposable})">
            <summary>
            This pins memory that is contained within DenseTensor.
            </summary>
            <param name="node">NodeOnnxValue containing DenseTensor</param>
            <param name="elementMeta"></param>
            <param name="disposables">cleanup list</param>
            <returns></returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException"></exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ManagedTypeProjection.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked by Dispose()</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MapHelper">
            <summary>
            The class holds keys and values for the dictionary
            in a for of two DenseTensors. The class is used to avoid
            data copy and make these available to the native code.
            Strings require special handling.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.NamedOnnxValue" -->
        <member name="F:Microsoft.ML.OnnxRuntime.NamedOnnxValue._value">
            <summary>
            Managed Tensor, Dictionary or IList
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.NamedOnnxValue._name">
            <summary>
            Name of the instance, model input/output
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.#ctor(System.String,System.Object)">
            <summary>
            Constructs an instance of NamedOnnxValue and represents
            a model input to an inference session.
            </summary>
            <param name="name">input/output name</param>
            <param name="value">Object that may be a tensor, Dictionary, IList</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.OnnxValueType)">
            <summary>
            Constructs an instance that contains a tensor, sequence or optional type.
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="valueType"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.#ctor(System.String,System.Object,Microsoft.ML.OnnxRuntime.MapHelper)">
            <summary>
            Use this to construct maps
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <param name="helper"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NamedOnnxValue.ValueType">
            <summary>
            Onnx Value Type if known. In general, NamedOnnxValue is able to contain
            arbitrary objects. Please, follow the convention described in the class doc.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.CreateFromTensor``1(System.String,Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0})">
            <summary>
            This is a factory method that instantiates NamedOnnxValue
            and associated name with an instance of a Tensor<typeparamref name="T"/>
            </summary>
            <typeparam name="T"></typeparam>
            <param name="name">name</param>
            <param name="value">Tensor<typeparamref name="T"/></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.CreateFromSequence``1(System.String,System.Collections.Generic.IEnumerable{``0})">
            <summary>
            This is a factory method that instantiates NamedOnnxValue.
            It would contain a sequence of elements
            </summary>
            <param name="name"></param>
            <param name="value"></param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.CreateFromMap``2(System.String,System.Collections.Generic.IDictionary{``0,``1})" -->
        <member name="P:Microsoft.ML.OnnxRuntime.NamedOnnxValue.Name">
            <summary>
            Exposes the name of the of the model input/output
            </summary>
            <value>name string</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NamedOnnxValue.Value">
            <summary>
            Exposes the underlying managed object
            </summary>
            <value>object</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.AsTensor``1">
            <summary>
            Try-get value as a Tensor&lt;T&gt;.
            </summary>
            <typeparam name="T">Type</typeparam>
            <returns>Tensor object if contained value is a Tensor. Null otherwise</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.AsEnumerable``1">
            <summary>
            Try-get value as an Enumerable&lt;T&gt;.
            T is usually a NamedOnnxValue instance that may contain
            Tensors, Sequences, Maps or optional types
            </summary>
            <typeparam name="T">Type</typeparam>
            <returns>Enumerable object if contained value is a Enumerable. Null otherwise</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.AsDictionary``2">
            <summary>
            Try-get value as an Dictionary&lt;K,V&gt;.
            </summary>
            <typeparam name="K">Key type currently primitive type only</typeparam>
            <typeparam name="V">Value type, currently primitive type only</typeparam>
            <returns>Dictionary object if contained value is a Dictionary. Null otherwise</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.InputToOrtValue(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Pin the underlying memory and create an instance of OrtValue containing a tensor
            based on the pinned managed memory. The caller is responsible for Disposing
            both OrtValue and pinnedMemoryHandle
            </summary>
            <param name="pinnedMemoryHandle">dispose after returned OrtValus is disposed</param>
            <returns>an instance of OrtValue. The lifespan of OrtValue must overlap pinnedMemoryHandle</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.OutputToOrtValue(Microsoft.ML.OnnxRuntime.NodeMetadata,System.IDisposable@)">
            <summary>
            Produces an output value for outputs. This produces an output value
            only for tensors or optional types that can contain a tensor.
            For all other Onnx value types, this method throws. Use Run() overloads
            that return DisposableNamedOnnxValue to get access to all Onnx value types
            that may be returned as output.
            </summary>
            <param name="metadata"></param>
            <param name="memoryOwner"></param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.GetDictionaryKeys" -->
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.NamedOnnxValue.GetDictionaryValues" -->
        <member name="M:Microsoft.ML.OnnxRuntime.NativeApiStatus.VerifySuccess(System.IntPtr)">
            <summary>
            Checks the native Status if the errocode is OK/Success. Otherwise constructs an appropriate exception and throws.
            Releases the native status object, as needed.
            </summary>
            <param name="nativeStatus"></param>
            <throws></throws>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateTensorRTProviderOptions">
            <summary>
            Creates native OrtTensorRTProviderOptions instance
            </summary>
            <param name="trtProviderOptionsInstance">(output) native instance of OrtTensorRTProviderOptions</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtUpdateTensorRTProviderOptions">
            <summary>
            Updates native OrtTensorRTProviderOptions instance using given key/value pairs
            </summary>
            <param name="trtProviderOptionsInstance">native instance of OrtTensorRTProviderOptions</param>
            <param name="providerOptionsKeys">configuration keys of OrtTensorRTProviderOptions</param>
            <param name="providerOptionsValues">configuration values of OrtTensorRTProviderOptions</param>
            <param name="numKeys">number of configuration keys</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetTensorRTProviderOptionsAsString">
            <summary>
            Get native OrtTensorRTProviderOptionsV2 in serialized string
            </summary>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="ptr">is a UTF-8 null terminated string allocated using 'allocator'</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseTensorRTProviderOptions">
            <summary>
            Releases native OrtTensorRTProviderOptions instance
            </summary>
            <param name="trtProviderOptionsInstance">native instance of OrtTensorRTProviderOptions to be released</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateCUDAProviderOptions">
            <summary>
            Creates native OrtCUDAProviderOptions instance
            </summary>
            <param name="cudaProviderOptionsInstance">(output) native instance of OrtCUDAProviderOptions</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtUpdateCUDAProviderOptions">
            <summary>
            Updates native OrtCUDAProviderOptions instance using given key/value pairs
            </summary>
            <param name="cudaProviderOptionsInstance">native instance of OrtCUDAProviderOptions</param>
            <param name="providerOptionsKeys">configuration keys of OrtCUDAProviderOptions</param>
            <param name="providerOptionsValues">configuration values of OrtCUDAProviderOptions</param>
            <param name="numKeys">number of configuration keys</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetCUDAProviderOptionsAsString">
            <summary>
            Get native OrtCUDAProviderOptionsV2 in serialized string
            </summary>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="ptr">is a UTF-8 null terminated string allocated using 'allocator'</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseCUDAProviderOptions">
            <summary>
            Releases native OrtCUDAProviderOptions instance
            </summary>
            <param name="cudaProviderOptionsInstance">native instance of OrtCUDAProviderOptions to be released</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateSessionWithPrepackedWeightsContainer">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="environment">Native OrtEnv instance</param>
            <param name="modelPath">UTF-8 bytes corresponding to model string path</param>
            <param name="sessionOptions">Native SessionOptions instance</param>
            <param name="prepackedWeightsContainer">Native OrtPrepackedWeightsContainer instance</param>
            <param name="session">(Output) Created native OrtSession instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateSessionFromArrayWithPrepackedWeightsContainer">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="environment">Native OrtEnv instance</param>
            <param name="modelData">Byte array correspoonding to the model</param>
            <param name="modelSize">Size of the model in bytes</param>
            <param name="sessionOptions">Native SessionOptions instance</param>
            <param name="prepackedWeightsContainer">Native OrtPrepackedWeightsContainer instance</param>
            <param name="session">(Output) Created native OrtSession instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddSessionConfigEntry">
            <summary>
            Add session config entry
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="configKey">Config key</param>
            <param name="configValue">Config value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeMethods.OrtSessionOptionsAppendExecutionProvider_CPU(System.IntPtr,System.Int32)">
            **
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_TensorRT">
            <summary>
            Append a TensorRT EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="trtProviderOptions">Native OrtTensorRTProviderOptions instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_TensorRT_V2">
            <summary>
            Append a TensorRT EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="trtProviderOptions">Native OrtTensorRTProviderOptionsV2 instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_CUDA">
            <summary>
            Append a CUDA EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="cudaProviderOptions">Native OrtCUDAProviderOptions instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider_CUDA_V2">
            <summary>
            Append a CUDA EP instance (configured based on given provider options) to the native OrtSessionOptions instance
            </summary>
            <param name="options">Native OrtSessionOptions instance</param>
            <param name="cudaProviderOptions">Native OrtCUDAProviderOptionsV2 instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddFreeDimensionOverride">
            <summary>
            Free Dimension override (by denotation)
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="dimDenotation">Dimension denotation</param>
            <param name="dimValue">Dimension value</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddFreeDimensionOverrideByName">
            <summary>
            Free Dimension override (by name)
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="dimName">Dimension name</param>
            <param name="dimValue">Dimension value</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtRegisterCustomOpsLibrary">
            <summary>
            Register custom op library
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="libraryPath">Library path</param>
            <param name="libraryHandle">(out) Native library handle</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtRegisterCustomOpsLibrary_V2">
            <summary>
            Register custom op library. ORT will manage freeing the library.
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="libraryPath">Library path</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddInitializer">
            <summary>
            Add initializer that is shared across Sessions using this SessionOptions (by denotation)
            </summary>
            <param name="options">Native SessionOptions instance</param>
            <param name="name">Name of the initializer</param>
            <param name="ortValue">Native OrtValue instnce</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DSessionOptionsAppendExecutionProvider">
             <summary>
             Append an execution provider instance to the native OrtSessionOptions instance.
            
             'SNPE' and 'XNNPACK' are currently supported as providerName values.
            
             The number of providerOptionsKeys must match the number of providerOptionsValues and equal numKeys.
             </summary>
             <param name="options">Native OrtSessionOptions instance</param>
             <param name="providerName">Execution provider to add.</param>
             <param name="providerOptionsKeys">Configuration keys to add</param>
             <param name="providerOptionsValues">Configuration values to add</param>
             <param name="numKeys">Number of configuration keys</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAddRunConfigEntry">
            <summary>
            Add run config entry
            </summary>
            <param name="options">Native RunOptions instance</param>
            <param name="configKey">Config key</param>
            <param name="configValue">Config value</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtMemoryInfoGetName">
            Do not free the returned value
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateArenaCfg">
            <summary>
            Create an instance of arena configuration which will be used to create an arena based allocator
            See docs/C_API.md for details on what the following parameters mean and how to choose these values
            </summary>
            <param name="maxMemory">Maximum amount of memory the arena allocates</param>
            <param name="arenaExtendStrategy">Strategy for arena expansion</param>
            <param name="initialChunkSizeBytes">Size of the region that the arena allocates first</param>
            <param name="maxDeadBytesPerChunk">Maximum amount of fragmentation allowed per chunk</param>
            <returns>Pointer to a native OrtStatus instance indicating success/failure of config creation</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseArenaCfg">
            <summary>
            Destroy an instance of an arena configuration instance
            </summary>
            <param name="arenaCfg">arena configuration instance to be destroyed</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateAllocator">
            <summary>
            Create an instance of allocator according to mem_info
            </summary>
            <param name="session">Session that this allocator should be used with</param>
            <param name="info">memory allocator specs</param>
            <param name="allocator">out pointer to a new allocator instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseAllocator">
            <summary>
            Destroy an instance of an allocator created by OrtCreateAllocator
            </summary>
            <param name="allocator">instance to be destroyed</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAllocatorAlloc">
            <summary>
            Allocate  a chunk of native memory
            </summary>
            <param name="allocator">allocator instance</param>
            <param name="size">bytes to allocate</param>
            <param name="p">out pointer to the allocated memory. Must be freed by OrtAllocatorFree</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtAllocatorFree">
            <summary>
            Release native memory allocated by an allocator
            </summary>
            <param name="allocator">allocator instance</param>
            <param name="p">pointer to native memory allocated by the allocator instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateIoBinding">
            <summary>
            Create OrtIoBinding instance that is used to bind memory that is allocated
            either by a 3rd party allocator or an ORT device allocator. Such memory should be wrapped by
            a native OrtValue of Tensor type. By binding such named values you will direct ORT to read model inputs
            and write model outputs to the supplied memory.
            </summary>
            <param name="session">session to create OrtIoBinding instance</param>
            <param name="io_binding">out a new instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseIoBinding">
            <summary>
            Destroy OrtIoBinding instance created by OrtCreateIoBinding
            </summary>
            <param name="io_bidning">instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtBindInput">
            <summary>
            Bind OrtValue to the model input with the specified name
            If binding with the specified name already exists, it will be replaced
            </summary>
            <param name="io_bidning">instance of OrtIoBinding</param>
            <param name="name">model input name (utf-8)</param>
            <param name="ort_value">OrtValue that is used for input (may wrap arbitrary memory).
                 The param instance is copied internally so this argument may be released.
            </param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSynchronizeBoundInputs">
            <summary>
            The API calls Sync() on all EP providers present. This blocks until the device has completed
            all preceding requested tasks. This is necessary when memory synchronization is required.
            For example, the memory bound to an input is likely to be on a different CUDA stream.
            For some scenarios and devices this may be a no-op, use
            your best judgment.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
            <returns>An instance of OrtStatus or null</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtBindOutput">
            <summary>
            Bind OrtValue to the model output with the specified name
            If binding with the specified name already exists, it will be replaced
            </summary>
            <param name="io_bidning">instance of OrtIoBinding</param>
            <param name="name">model output name (utf-8)</param>
            <param name="ort_value">OrtValue that is used for output (may wrap arbitrary memory).
                 The param instance is copied internally so this argument may be released.
            </param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtBindOutputToDevice">
            <summary>
            Bind a device to the model output with the specified name
            This is useful when the OrtValue can not be allocated ahead of time
            due to unknown dimensions.
            </summary>
            <param name="io_binding">Instance of OrtIoBinding</param>
            <param name="name">UTF-8 zero terminated name</param>
            <param name="mem_info">OrtMemoryInfo instance that contains device id. May be obtained from the device specific allocator instance</param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSynchronizeBoundOutputs">
            <summary>
            The API calls Sync() on all EP providers present. This blocks until the device has completed
            all preceding requested tasks. This is necessary when memory synchronization is required.
            For some scenarios and devices this may be a no-op, use your best judgment.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
            <returns>An instance of OrtStatus or null</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetBoundOutputNames">
            <summary>
            The function will return all bound output names in the order they were bound.
            It is the same order that the output values will be returned after RunWithBinding() is used.
            The function will allocate two native allocations  using the allocator supplied.
            The caller is responsible for deallocating both of the buffers using the same allocator.
            You may use OrtMemoryAllocation disposable class to wrap those allocations.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
            <param name="allocator">allocator to use for memory allocation</param>
            <param name="buffer">a continuous buffer that contains all output names.
            Names are not zero terminated use lengths to extract strings. This needs to be deallocated.</param>
            <param name="lengths">A buffer that contains lengths (size_t) for each of the returned strings in order.
            The buffer must be deallocated.</param>
            <param name="count">this contains the count of names returned which is the number of elements in lengths.</param>
            <returns></returns>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetBoundOutputValues" -->
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtClearBoundInputs">
            <summary>
            Clears Input bindings. This is a convenience method.
            Releasing OrtIoBinding instance would clear all bound inputs.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtClearBoundOutputs">
            <summary>
            Clears Output bindings. This is a convenience method.
            Releasing OrtIoBinding instance would clear all bound outputs.
            </summary>
            <param name="io_binding">instance of OrtIoBinding</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtTensorAt">
            <summary>
            Provides element-level access into a tensor.
            </summary>
            <param name="location_values">a pointer to an array of index values that specify an element's location in the tensor data blob</param>
            <param name="location_values_count">length of location_values</param>
            <param name="out">a pointer to the element specified by location_values</param>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreateAndRegisterAllocator" -->
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSetLanguageProjection">
            <summary>
            Set the language projection for collecting telemetry data when Env is created
            </summary>
            <param name="projection">the source projected language</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtSessionGetModelMetadata">
            <summary>
            Gets the ModelMetadata associated with an InferenceSession
            </summary>
            <param name="session">instance of OrtSession</param>
            <param name="modelMetadata">(output) instance of OrtModelMetadata</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetProducerName">
            <summary>
            Gets the producer name associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) producer name from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetGraphName">
            <summary>
            Gets the graph name associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) graph name from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetDomain">
            <summary>
            Gets the domain associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) domain from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetDescription">
            <summary>
            Gets the description associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) description from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetGraphDescription">
            <summary>
            Gets the description associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="value">(output) graph description from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetVersion">
            <summary>
            Gets the version associated with a ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="value">(output) version from the ModelMetadata instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataGetCustomMetadataMapKeys">
            <summary>
            Gets all the keys in the custom metadata map in the ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="keys">(output) all keys in the custom metadata map</param>
            <param name="numKeys">(output) number of keys in the custom metadata map</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtModelMetadataLookupCustomMetadataMap">
            <summary>
            Gets the value associated with the given key in custom metadata map in the ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
            <param name="allocator">instance of OrtAllocator</param>
            <param name="key">key in the custom metadata map</param>
            <param name="value">(output) value for the key in the custom metadata map</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseModelMetadata">
            <summary>
            Frees ModelMetadata instance
            </summary>
            <param name="modelMetadata">instance of OrtModelMetadata</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetTensorMutableData">
            This function doesn't work with string tensor
            this is a no-copy method whose pointer is only valid until the backing OrtValue* is free'd.
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtFillStringTensor">
            \param value A tensor created from OrtCreateTensor... function.
            \param len total data length, not including the trailing '\0' chars.
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetSymbolicDimensions">
            Get the symbolic dimension names for dimensions with a value of -1.
            Order and number of entries is the same as values returned by GetDimensions.
            The name may be empty for an unnamed symbolic dimension.
            e.g.
            If OrtGetDimensions returns [-1, -1, 2], OrtGetSymbolicDimensions would return an array with 3 entries.
            If the values returned were ['batch', '', ''] it would indicate that
             - the first dimension was a named symbolic dimension (-1 dim value and name in symbolic dimensions),
             - the second dimension was an unnamed symbolic dimension (-1 dim value and empty string),
             - the entry for the third dimension should be ignored as it is not a symbolic dimension (dim value >= 0).
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetTensorShapeElementCount">
            How many elements does this tensor have.
            May return a negative value
            e.g.
            [] -> 1
            [1,3,4] -> 12
            [2,0,4] -> 0
            [-1,3,4] -> -1
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DCastTypeInfoToMapTypeInfo">
             Map Type API
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtGetAvailableProviders">
            <summary>
            Queries all the execution providers supported in the native onnxruntime shared library
            </summary>
            <param name="providers">(output) all execution providers (strings) supported in the native onnxruntime shared library</param>
            <param name="numProviders">(output) number of execution providers (strings)</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleaseAvailableProviders">
            <summary>
            Releases all execution provider strings allocated and returned by OrtGetAvailableProviders
            </summary>
            <param name="providers">all execution providers (strings) returned by OrtGetAvailableProviders</param>
            <param name="numProviders">number of execution providers (strings)</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtCreatePrepackedWeightsContainer">
            <summary>
            Create an instance of PrepackedWeightsContainer
            </summary>
            <param name="prepackedWeightsContainer">(output) Created native OrtPrepackedWeightsContainer instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeMethods.DOrtReleasePrepackedWeightsContainer">
            <summary>
            Destroy an instance of PrepackedWeightsContainer
            </summary>
            <param name="prepackedWeightsContainer">Native OrtPrepackedWeightsContainer instance to be destroyed</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper">
            <summary>
            This helper class contains methods to create native OrtValue from a managed value object
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringToZeroTerminatedUtf8(System.String)">
            <summary>
            Converts C# UTF-16 string to UTF-8 zero terminated
            byte[] instance
            </summary>
            <param name="s">string to be converted</param>
            <returns>UTF-8 encoded equivalent</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringToUtf8NativeMemory(System.String,System.IntPtr,System.Int32)">
            <summary>
            This function converts the input string into UTF-8 encoding string (no zero termination)
            straight into the pre-allocated native buffer. The buffer size
            must match the required size and can be obtained in advance with
            System.Text.Encoding.UTF8.GetByteCount(s).
            
            The function is helpful when we populate native string tensor buffers directly where
            the elements stored do not have zero terminator.
            </summary>
            <param name="s">managed string</param>
            <param name="ptr">natively allocated buffer</param>
            <param name="totalBytesToWrite">pre-allocated buffer size</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringFromNativeUtf8(System.IntPtr,Microsoft.ML.OnnxRuntime.OrtAllocator)">
            <summary>
            Reads UTF-8 encode string from a C zero terminated string
            and converts it into a C# UTF-16 encoded string
            </summary>
            <param name="nativeUtf8">pointer to native or pinned memory where Utf-8 resides</param>
            <param name="allocator">optional allocator to free nativeUtf8 if it was allocated by OrtAllocator</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.StringAndUtf8FromNative(Microsoft.ML.OnnxRuntime.OrtAllocator,System.IntPtr,System.String@,System.IntPtr@)">
            <summary>
            Reads UTF-8 string from native C zero terminated string,
            makes a copy of it on unmanaged heap and converts it to C# UTF-16 string,
            then returns both C# string and the unmanaged copy of the UTF-8 string.
            
            On return it deallocates the nativeUtf8 string using the specified allocator
            </summary>
            <param name="allocator">allocator to use to free nativeUtf8</param>
            <param name="nativeUtf8">input</param>
            <param name="str">C# UTF-16 string</param>
            <param name="utf8">UTF-8 bytes in a unmanaged allocation, zero terminated</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.NativeOnnxValueHelper.GetPlatformSerializedString(System.String)">
            <summary>
            Converts C# UTF-16 string to UTF-8 zero terminated
            byte[] instance
            </summary>
            <param name="str">string to be converted</param>
            <returns>UTF-8 encoded equivalent</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MarshaledString">
            <summary>
            This class converts a string to a UTF8 encoded byte array and then copies it to an unmanaged buffer.
            This is done, so we can pass it to the native code and avoid pinning.
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.MarshaledString.Length" -->
        <member name="P:Microsoft.ML.OnnxRuntime.MarshaledString.Value">
            <summary>
            Actual native buffer
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.MarshaledString.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.MarshaledStringArray">
            <summary>
            Keeps a list of MarshaledString instances and provides a way to dispose them all at once.
            It is a ref struct, so it can not be IDisposable.
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ProviderOptionsUpdater">
            <summary>
            Utility class used in SessioniOptions and ProviderOptions
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ProviderOptionsUpdater.Update(System.Collections.Generic.Dictionary{System.String,System.String},System.IntPtr,System.Func{System.IntPtr,System.IntPtr[],System.IntPtr[],System.UIntPtr,System.IntPtr})">
            <summary>
            A utility method to update the provider options, provides common functionality.
            
            </summary>
            <param name="providerOptions">The actual key/value option pairs</param>
            <param name="handle">to the object</param>
            <param name="updateFunc">encapsulates a native method that returns 
            Arg1=handle, Arg2=array of keys, Arg3=array of values, Arg4 - count, Arg5 - return ORT status</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtAllocatorType">
            <summary>
            See documentation for OrtAllocatorType in C API
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtMemType">
            <summary>
            See documentation for OrtMemType in C API
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtArenaCfg">
            <summary>
            This class encapsulates arena configuration information that will be used to define the behavior
            of an arena based allocator
            See docs/C_API.md for more details
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtArenaCfg.#ctor(System.UInt32,System.Int32,System.Int32,System.Int32)">
            <summary>
            Create an instance of arena configuration which will be used to create an arena based allocator
            See docs/C_API.md for details on what the following parameters mean and how to choose these values
            </summary>
            <param name="maxMemory">Maximum amount of memory the arena allocates</param>
            <param name="arenaExtendStrategy">Strategy for arena expansion</param>
            <param name="initialChunkSizeBytes">Size of the region that the arena allocates first</param>
            <param name="maxDeadBytesPerChunk">Maximum amount of fragmentation allowed per chunk</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtArenaCfg.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtArenaCfg.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtEnv
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtMemoryInfo">
            <summary>
            This class encapsulates and most of the time owns the underlying native OrtMemoryInfo instance.
            Instance returned from OrtAllocator will not own OrtMemoryInfo, the class must be disposed
            regardless.
            
            Use this class to query and create OrtAllocator instances so you can pre-allocate memory for model
            inputs/outputs and use it for binding. Instances of the class can also used to created OrtValues bound
            to pre-allocated memory. In that case, the instance of OrtMemoryInfo contains the information about the allocator
            used to allocate the underlying memory.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.DefaultInstance">
            <summary>
            Default CPU based instance
            </summary>
            <value>Singleton instance of a CpuMemoryInfo</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.#ctor(System.IntPtr,System.Boolean)">
            <summary>
            This allocator takes an native pointer to already existing
            instance of OrtMemoryInfo. That instance may either be owned or not
            owned. In the latter case, this class serves to expose native properties
            of the instance.
            </summary>
            <param name="allocInfo"></param>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorCPU">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorCUDA">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.allocatorCUDA_PINNED">
            <summary>
            Predefined utf8 encoded allocator names. Use them to construct an instance of
            OrtMemoryInfo to avoid UTF-16 to UTF-8 conversion costs.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.#ctor(System.Byte[],Microsoft.ML.OnnxRuntime.OrtAllocatorType,System.Int32,Microsoft.ML.OnnxRuntime.OrtMemType)">
            <summary>
            Create an instance of OrtMemoryInfo according to the specification
            Memory info instances are usually used to get a handle of a native allocator
            that is present within the current inference session object. That, in turn, depends
            of what execution providers are available within the binary that you are using and are
            registered with Add methods.
            </summary>
            <param name="utf8AllocatorName">Allocator name. Use of the predefined above.</param>
            <param name="allocatorType">Allocator type</param>
            <param name="deviceId">Device id</param>
            <param name="memoryType">Memory type</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.#ctor(System.String,Microsoft.ML.OnnxRuntime.OrtAllocatorType,System.Int32,Microsoft.ML.OnnxRuntime.OrtMemType)">
            <summary>
            Create an instance of OrtMemoryInfo according to the specification.
            </summary>
            <param name="allocatorName">Allocator name</param>
            <param name="allocatorType">Allocator type</param>
            <param name="deviceId">Device id</param>
            <param name="memoryType">Memory type</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Name">
            <summary>
            Name of the allocator associated with the OrtMemoryInfo instance
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Id">
            <summary>
            Returns device ID
            </summary>
            <value>returns integer Id value</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.GetMemoryType">
            <summary>
             The below 2 are really properties but naming them is a challenge
             as names would conflict with the returned type. Also, there are native
             calls behind them so exposing them as Get() would be appropriate.
            </summary>
            <returns>OrtMemoryType for the instance</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.GetAllocatorType">
            <summary>
            Fetches allocator type from the underlying OrtAllocator
            </summary>
            <returns>Returns allocator type</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Equals(System.Object)">
            <summary>
            Overrides System.Object.Equals(object)
            </summary>
            <param name="obj">object to compare to</param>
            <returns>true if obj is an instance of OrtMemoryInfo and is equal to this</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.Equals(Microsoft.ML.OnnxRuntime.OrtMemoryInfo)">
            <summary>
            Compares this instance with another
            </summary>
            <param name="other">OrtMemoryInfo to compare to</param>
            <returns>true if instances are equal according to OrtCompareMemoryInfo.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.GetHashCode">
            <summary>
            Overrides System.Object.GetHashCode()
            </summary>
            <returns>integer hash value</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryInfo.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtMmeoryInfo
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtExternalAllocation">
            <summary>
            This class represents an arbitrary buffer of memory
            allocated and owned by the user. It can be either a CPU, GPU or other device memory
            that can be suitably represented by IntPtr.
            This is just a composite of the buffer related information.
            The memory is assumed to be pinned if necessary and usable immediately
            in the native code.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.#ctor(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,System.Int64[],Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.IntPtr,System.Int64)">
            <summary>
            Constructor
            </summary>
            <param name="memInfo">use to accurately describe a piece of memory that this is wrapping</param>
            <param name="shape">shape of this buffer</param>
            <param name="elementType">element type</param>
            <param name="pointer">the actual pointer to memory</param>
            <param name="sizeInBytes">size of the allocation in bytes</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Info">
            <summary>
            OrtMemoryInfo
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Shape">
            <summary>
            Shape
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.ElementType">
            <summary>
            Data type
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Pointer">
            <summary>
            Actual memory ptr
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtExternalAllocation.Size">
            <summary>
            Size of the allocation in bytes
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation">
            <summary>
            This class represents memory allocation made by a specific onnxruntime
            allocator. Use OrtAllocator.Allocate() to obtain an instance of this class.
            It implements IDisposable and makes use of the original allocator
            used to allocate the memory. The lifespan of the allocator instance must eclipse the
            lifespan of the allocation. Or, if you prefer, all OrtMemoryAllocation instances must be
            disposed of before the corresponding allocator instances are disposed of.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.#ctor(Microsoft.ML.OnnxRuntime.OrtAllocator,System.IntPtr,System.UInt32)">
            <summary>
            This constructs an instance representing an native memory allocation.
            Typically returned by OrtAllocator.Allocate(). However, some APIs return
            natively allocated IntPtr using a specific allocator. It is a good practice
            to wrap such a memory into OrtAllocation for proper disposal. You can set
            size to zero if not known, which is not important for disposing.
            </summary>
            <param name="allocator"></param>
            <param name="pointer"></param>
            <param name="size"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.Pointer">
            <summary>
            Internal accessor to call native methods
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.Size">
            <summary>
            Size of the allocation
            </summary>
            <value>uint size of the allocation in bytes</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.Info">
            <summary>
            Memory Information about this allocation
            </summary>
            <value>Returns OrtMemoryInfo from the allocator</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtMemoryAllocation.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to deallocate
            a chunk of memory using the specified allocator.
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtAllocator">
            <summary>
            The class exposes native internal allocator for Onnxruntime.
            This allocator enables you to allocate memory from the internal
            memory pools including device allocations. Useful for binding.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtAllocator.DefaultInstance">
            <summary>
            Default CPU allocator instance
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtAllocator.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.#ctor(System.IntPtr,System.Boolean)">
            <summary>
            Internal constructor wraps existing native allocators
            </summary>
            <param name="allocator"></param>
            <param name="owned"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.#ctor(Microsoft.ML.OnnxRuntime.InferenceSession,Microsoft.ML.OnnxRuntime.OrtMemoryInfo)">
            <summary>
            Creates an instance of OrtAllocator according to the specifications in OrtMemorInfo.
            The requested allocator should be available within the given session instance. This means
            both, the native library was build with specific allocators (for instance CUDA) and the corresponding
            provider was added to SessionsOptions before instantiating the session object.
            </summary>
            <param name="session"></param>
            <param name="memInfo"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtAllocator.Info">
            <summary>
            OrtMemoryInfo instance owned by the allocator
            </summary>
            <value>Instance of OrtMemoryInfo describing this allocator</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.Allocate(System.UInt32)">
            <summary>
            Allocate native memory. Returns a disposable instance of OrtMemoryAllocation
            </summary>
            <param name="size">number of bytes to allocate</param>
            <returns>Instance of OrtMemoryAllocation</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.FreeMemory(System.IntPtr)">
            <summary>
            This internal interface is used for freeing memory.
            </summary>
            <param name="allocation">pointer to a native memory chunk allocated by this allocator instance</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtAllocator.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtAllocator
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.DOrtLoggingFunction">
            <summary>
            Delegate for logging function callback.
            Supply your function and register it with the environment to receive logging callbacks via
            EnvironmentCreationOptions
            </summary>
            <param name="param">Pointer to data passed into Constructor `log_param` parameter.</param>
            <param name="severity">Log severity level.</param>
            <param name="category">Log category</param>
            <param name="logId">Log Id.</param>
            <param name="codeLocation">Code location detail.</param>
            <param name="message">Log message.</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions">
            <summary>
            Options you might want to supply when creating the environment.
            Everything is optional.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.logId">
            <summary>
             Supply a log id to identify the application using ORT, otherwise, a default one will be used
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.logLevel">
            <summary>
            Initial logging level so that you can see what is going on during environment creation
            Default is LogLevel.Warning
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.threadOptions">
            <summary>
            Supply OrtThreadingOptions instance, otherwise null
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.loggingParam">
            <summary>
            Supply IntPtr logging param when registering logging function, otherwise IntPtr.Zero
            This param will be passed to the logging function when called, it is opaque for the API
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions.loggingFunction">
            <summary>
            Supply custom logging function otherwise null
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtEnv">
            <summary>
            The singleton class OrtEnv contains the process-global ONNX Runtime environment.
            It sets up logging, creates system wide thread-pools (if Thread Pool options are provided)
            and other necessary things for OnnxRuntime to function. 
            
            Create or access OrtEnv by calling the Instance() method. Instance() can be called multiple times.
            It would return the same instance.
            
            CreateInstanceWithOptions() provides a way to create environment with options.
            It must be called once before Instance() is called, otherwise it would not have effect.
            
            If the environment is not explicitly created, it will be created as needed, e.g.,
            when creating a SessionOptions instance.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.#ctor(System.IntPtr,Microsoft.ML.OnnxRuntime.OrtLoggingLevel)">
            <summary>
            The only __ctor__ for OrtEnv.
            </summary>
            <param name="handle"></param>
            <param name="logLevel"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.LoggingFunctionThunk(System.IntPtr,System.IntPtr,System.IntPtr,System.IntPtr,System.IntPtr,System.IntPtr)">
            <summary>
            The actual logging callback to the native code
            </summary>
            <param name="param"></param>
            <param name="severity"></param>
            <param name="category"></param>
            <param name="logid"></param>
            <param name="code_location"></param>
            <param name="message"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.CreateInstance">
            <summary>
            This is invoked only once when the first call refers _instance.Value.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.SetLanguageProjection(Microsoft.ML.OnnxRuntime.OrtEnv)">
            <summary>
            To be called only from constructor
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.Instance">
            <summary>
            Instantiates (if not already done so) a new OrtEnv instance with the default logging level
            and no other options. Otherwise returns the existing instance.
            
            It returns the same instance on every call - `OrtEnv` is singleton
            </summary>
            <returns>Returns a singleton instance of OrtEnv that represents native OrtEnv object</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.CreateInstanceWithOptions(Microsoft.ML.OnnxRuntime.EnvironmentCreationOptions@)">
            <summary>
            Provides a way to create an instance with options.
            It throws if the instance already exists and the specified options
            not have effect.
            </summary>
            <param name="options"></param>
            <returns></returns>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException">if the singleton has already been created</exception>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.IsCreated">
            <summary>
            Provides visibility if singleton already been instantiated
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.EnableTelemetryEvents">
            <summary>
            Enable platform telemetry collection where applicable
            (currently only official Windows ORT builds have telemetry collection capabilities)
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.DisableTelemetryEvents">
            <summary>
            Disable platform telemetry collection
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.CreateAndRegisterAllocator(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,Microsoft.ML.OnnxRuntime.OrtArenaCfg)">
            <summary>
            Create and register an allocator to the OrtEnv instance
            so as to enable sharing across all sessions using the OrtEnv instance
            <param name="memInfo">OrtMemoryInfo instance to be used for allocator creation</param>
            <param name="arenaCfg">OrtArenaCfg instance that will be used to define the behavior of the arena based allocator</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.GetVersionString">
            <summary>
            This function returns the onnxruntime version string
            </summary>
            <returns>version string</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.GetAvailableProviders">
            <summary>
            Queries all the execution providers supported in the native onnxruntime shared library
            </summary>
            <returns>an array of strings that represent execution provider names</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.EnvLogLevel">
            <summary>
            Get/Set log level property of OrtEnv instance
            Default LogLevel.Warning
            </summary>
            <returns>env log level</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.Handle">
            <summary>
            Returns a handle to the native `OrtEnv` instance held by the singleton C# `OrtEnv` instance
            Exception caching: May throw an exception on every call, if the `OrtEnv` constructor threw an exception
            during lazy initialization
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtEnv.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtEnv.ReleaseHandle">
            <summary>
            Destroys native object
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtIoBinding">
             <summary>
             This class enables binding of inputs and/or outputs to pre-allocated
             memory. This enables interesting scenarios. For example, if your input
             already resides in some pre-allocated memory like GPU, you can bind
             that piece of memory to an input name and shape and onnxruntime will use that as input.
             Other traditional inputs can also be bound that already exists as Tensors.
            
             Note, that this arrangement is designed to minimize data copies and to that effect
             your memory allocations must match what is expected by the model, whether you run on
             CPU or GPU. Data copy will still be made, if your pre-allocated memory location does not
             match the one expected by the model. However, copies with OrtIoBindings are only done once,
             at the time of the binding, not at run time. This means, that if your input data required a copy,
             your further input modifications would not be seen by onnxruntime unless you rebind it, even if it is
             the same buffer. If you require the scenario where data is copied, OrtIOBinding may not be the best match
             for your use case.
            
             The fact that data copy is not made during runtime also has performance implications.
             </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.#ctor(Microsoft.ML.OnnxRuntime.InferenceSession)">
            <summary>
            Use InferenceSession.CreateIoBinding()
            </summary>
            <param name="session"></param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtIoBinding.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInput(System.String,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],Microsoft.ML.OnnxRuntime.OrtMemoryAllocation)">
            <summary>
            Bind a piece of pre-allocated native memory as a OrtValue Tensor with a given shape
            to an input with a given name. The model will read the specified input from that memory
            possibly avoiding the need to copy between devices. OrtMemoryAllocation continues to own
            the chunk of native memory, and the allocation should be alive until the end of execution.
            </summary>
            <param name="name">of the input</param>
            <param name="elementType">Tensor element type</param>
            <param name="shape"></param>
            <param name="allocation">native memory allocation</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInput(System.String,Microsoft.ML.OnnxRuntime.OrtExternalAllocation)">
            <summary>
            Bind externally (not from OrtAllocator) allocated memory as input.
            The model will read the specified input from that memory
            possibly avoiding the need to copy between devices. The user code continues to own
            the chunk of externally allocated memory, and the allocation should be alive until the end of execution.
            </summary>
            <param name="name">name</param>
            <param name="allocation">non ort allocated memory</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInput(System.String,Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue)">
            <summary>
            Bind the input with the given name as an OrtValue Tensor allocated in pinned managed memory.
            Instance of FixedBufferOnnxValue owns the memory and should be alive until the end of execution.
            </summary>
            <param name="name">name of input</param>
            <param name="fixedValue"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.SynchronizeBoundInputs">
            <summary>
            Blocks until device completes all preceding requested tasks.
            Useful for memory synchronization.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutput(System.String,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],Microsoft.ML.OnnxRuntime.OrtMemoryAllocation)">
            <summary>
            Bind model output to an OrtValue as Tensor with a given type and shape. An instance of OrtMemoryAllocaiton
            owns the memory and should be alive for the time of execution.
            </summary>
            <param name="name">of the output</param>
            <param name="elementType">tensor element type</param>
            <param name="shape">tensor shape</param>
            <param name="allocation">allocated memory</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutput(System.String,Microsoft.ML.OnnxRuntime.OrtExternalAllocation)">
            <summary>
            Bind externally (not from OrtAllocator) allocated memory as output.
            The model will read the specified input from that memory
            possibly avoiding the need to copy between devices. The user code continues to own
            the chunk of externally allocated memory, and the allocation should be alive until the end of execution.
            </summary>
            <param name="name">name</param>
            <param name="allocation">non ort allocated memory</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutput(System.String,Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue)">
            <summary>
            Bind model output to a given instance of FixedBufferOnnxValue which owns the underlying
            pinned managed memory and should be alive for the time of execution.
            </summary>
            <param name="name">of the output</param>
            <param name="fixedValue"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOutputToDevice(System.String,Microsoft.ML.OnnxRuntime.OrtMemoryInfo)">
            <summary>
            This function will bind model output with the given name to a device
            specified by the memInfo.
            </summary>
            <param name="name">output name</param>
            <param name="memInfo">instance of memory info</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.SynchronizeBoundOutputs">
            <summary>
            Blocks until device completes all preceding requested tasks.
            Useful for memory synchronization.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindOrtAllocation(System.String,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],Microsoft.ML.OnnxRuntime.OrtMemoryAllocation,System.Boolean)">
            <summary>
            Bind allocation obtained from an Ort allocator
            </summary>
            <param name="name">name </param>
            <param name="elementType">data type</param>
            <param name="shape">tensor shape</param>
            <param name="allocation">ort allocation</param>
            <param name="isInput">whether this is input or output</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindExternalAllocation(System.String,Microsoft.ML.OnnxRuntime.OrtExternalAllocation,System.Boolean)">
            <summary>
            Bind external allocation as input or output.
            The allocation is owned by the user code.
            </summary>
            <param name="name">name </param>
            <param name="allocation">non ort allocated memory</param>
            <param name="isInput">whether this is an input or output</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.BindInputOrOutput(System.String,System.IntPtr,System.Boolean)">
            <summary>
            Internal helper
            </summary>
            <param name="name"></param>
            <param name="ortValue"></param>
            <param name="isInput"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.GetOutputNames">
            <summary>
            Returns an array of output names in the same order they were bound
            </summary>
            <returns>array of output names</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "M:Microsoft.ML.OnnxRuntime.OrtIoBinding.GetOutputValues" -->
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.ClearBoundInputs">
            <summary>
            Clear all bound inputs and start anew
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.ClearBoundOutputs">
            <summary>
            Clear all bound outputs
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtIoBinding.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtIoBidning
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtLoggingLevel">
            <summary>
            Log severity levels
            
            Must in sync with OrtLoggingLevel in onnxruntime_c_api.h
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtThreadingOptions">
            <summary>
            This class allows to specify global thread pool options
            when instantiating the ONNX Runtime environment for the first time.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.Handle">
            <summary>
            A pointer to a underlying native instance of ThreadingOptions
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.#ctor">
            <summary>
            Create an empty threading options.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.GlobalInterOpNumThreads">
            <summary>
            Set global inter-op thread count.
            Setting it to 0 will allow ORT to choose the number of threads used for parallelization of
            multiple kernels. Setting it to 1 will cause the main thread to be used (i.e., no thread pools will be used).
            This setting is only meaningful when the execution mode is set to Parallel.
            </summary>
            <param name="value">The number of threads.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.GlobalIntraOpNumThreads">
            <summary>
            Sets the number of threads available for intra-op parallelism (i.e. within a single op).
            Setting it to 0 will allow ORT to choose the number of threads, setting it to 1 will cause the main thread to be used (i.e., no thread pools will be used).
            </summary>
            <param name="value">The number of threads.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.GlobalSpinControl">
            <summary>
            Allows spinning of thread pools when their queues are empty in anticipation of imminent task arrival.
            This call sets the value for both inter-op and intra-op thread pools.
            If the CPU usage is very high then do not enable this.
            </summary>
            <param name="value">If true allow the thread pools to spin.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.SetGlobalDenormalAsZero">
            <summary>
            When this is set it causes intra-op and inter-op thread pools to flush denormal values to zero.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of ThreadingOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtThreadingOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OnnxValueType">
            <summary>
            A type of data that OrtValue encapsulates.
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtValue">
            <summary>
            Represents a disposable OrtValue.
            This class exposes a native instance of OrtValue.
            The class implements IDisposable via SafeHandle and must
            be disposed.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.#ctor(System.IntPtr,System.Boolean)">
            <summary>
            Use factory methods to instantiate this class
            </summary>
            <param name="handle">Pointer to a native instance of OrtValue</param>
            <param name="owned">Default true, own the raw handle. Otherwise, the handle is owned by another instance
            However, we use this class to expose OrtValue that is owned by DisposableNamedOnnxValue
            </param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValue.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.Disown">
            <summary>
            This internal interface is used to transfer ownership elsewhere.
            This instance must still be disposed in case there are other native
            objects still owned. This is a convenience method to ensure that an underlying
            OrtValue is disposed exactly once when exception is thrown.
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateTensorValueWithData(Microsoft.ML.OnnxRuntime.OrtMemoryInfo,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int64[],System.IntPtr,System.Int64)">
            <summary>
            Factory method to construct an OrtValue of Tensor type on top of pre-allocated memory.
            This can be a piece of native memory allocated by OrtAllocator (possibly on a device)
            or a piece of pinned managed memory.
            
            The resulting OrtValue does not own the underlying memory buffer and will not attempt to
            deallocate it.
            </summary>
            <param name="memInfo">Memory Info. For managed memory it is a default cpu.
                                  For Native memory must be obtained from the allocator or OrtMemoryAllocation instance</param>
            <param name="elementType">DataType for the Tensor</param>
            <param name="shape">Tensor shape</param>
            <param name="dataBuffer">Pointer to a raw memory buffer</param>
            <param name="bufferLength">Buffer length in bytes</param>
            <returns>A disposable instance of OrtValue</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.CreateFromTensorObject(System.Object,System.Nullable{System.Buffers.MemoryHandle}@,Microsoft.ML.OnnxRuntime.Tensors.TensorElementType@)">
            <summary>
            This is a factory method creates a native Onnxruntime OrtValue containing a tensor.
            The method will attempt to pin managed memory so no copying occurs when data is passed down
            to native code.
            </summary>
            <param name="value">Tensor object</param>
            <param name="memoryHandle">For all tensor types but string tensors we endeavor to use managed memory
             to avoid additional allocation and copy. This out parameter represents a chunk of pinned memory which will need
             to be disposed when no longer needed. The lifespan of memoryHandle should eclipse the lifespan of the corresponding
             OrtValue.
            </param>
            <param name="elementType">discovered tensor element type</param>
            <returns>And instance of OrtValue constructed on top of the object</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValue.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtValue
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.IOrtValueOwner">
            <summary>
            Provides access from the underlying object that owns disposable OrtValue
            The returned value does not own the actual memory and does nothing on Dispose()
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeOrtValueCollectionOwner`1">
            <summary>
            This class is used in conjunction with DisposableNamedOnnxValue to 
            own native collection OrtValue and dispose of it along with any DisposableNamedOnnxValues
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.NativeOrtValueCollectionOwner`1.Value">
            <summary>
            Returns a non-owning ortValue
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.OrtValueTensor`1" -->
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.#ctor(Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            Constructs an instance and takes ownership of ortValue on success
            </summary>
            <param name="ortValue">ortValue that is a Tensor</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.Value">
            <summary>
            Returns a non-owning copy of OrtValue so the
            result can not release native memory
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.GetSpan">
            <summary>
            Used by MemoryManager to produce Memory Property
            </summary>
            <returns>SpanT</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtValueTensor`1.Pin(System.Int32)">
            <summary>
            Satisfy MemoryManager abstract implementation
            </summary>
            <param name="elementIndex"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer">
            <summary>
            This class holds pre-packed weights of shared initializers to be shared across sessions using these initializers
            and thereby provide memory savings by sharing the same pre-packed versions of these shared initializers
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.#ctor">
            <summary>
            Constructs an empty PrePackedWeightsContainer
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.Pointer">
            <summary>
            Internal accessor to call native methods
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.PrePackedWeightsContainer.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to deallocate
            a chunk of memory using the specified allocator.
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions">
            <summary>
            Holds the options for configuring a TensorRT Execution Provider instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.#ctor">
            <summary>
            Constructs an empty OrtTensorRTProviderOptions instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.GetOptions">
            <summary>
            Get TensorRT EP provider options
            </summary>
            <returns> return C# UTF-16 encoded string </returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.UpdateOptions(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Updates  the configuration knobs of OrtTensorRTProviderOptions that will eventually be used to configure a TensorRT EP
            Please refer to the following on different key/value pairs to configure a TensorRT EP and their meaning:
            https://onnxruntime.ai/docs/execution-providers/TensorRT-ExecutionProvider.html
            </summary>
            <param name="providerOptions">key/value pairs used to configure a TensorRT Execution Provider</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.GetDeviceId">
            <summary>
            Get device id of TensorRT EP.
            </summary>
            <returns> device id </returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtTensorRTProviderOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions">
            <summary>
            Holds the options for configuring a CUDA Execution Provider instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.#ctor">
            <summary>
            Constructs an empty OrtCUDAroviderOptions instance
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.GetOptions">
            <summary>
            Get CUDA EP provider options
            </summary>
            <returns> return C# UTF-16 encoded string </returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.UpdateOptions(System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Updates  the configuration knobs of OrtCUDAProviderOptions that will eventually be used to configure a CUDA EP
            Please refer to the following on different key/value pairs to configure a CUDA EP and their meaning:
            https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html
            </summary>
            <param name="providerOptions">key/value pairs used to configure a CUDA Execution Provider</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of OrtCUDAProviderOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ProviderOptionsValueHelper">
            <summary>
            This helper class contains methods to handle values of provider options
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.ProviderOptionsValueHelper.StringToDict(System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Parse from string and save to dictionary
            </summary>
            <param name="s">C# string</param>
            <param name="dict">Dictionary instance to store the parsing result of s</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.CoreMLFlags">
            <summary>
            CoreML flags for use with SessionOptions
            </summary>
            <see cref="!:https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/providers/coreml/coreml_provider_factory.h"/>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NnapiFlags">
            <summary>
            NNAPI flags for use with SessionOptions
            </summary>
            <see cref="!:https://github.com/microsoft/onnxruntime/blob/main/include/onnxruntime/core/providers/nnapi/nnapi_provider_factory.h"/>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.RunOptions">
            <summary>
             Sets various runtime options. 
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.RunOptions.#ctor">
            <summary>
            Default __ctor. Creates default RuntimeOptions
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.LogSeverityLevel">
            <summary>
            Log Severity Level for the session logs. Default = ORT_LOGGING_LEVEL_WARNING
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.LogVerbosityLevel">
            <summary>
            Log Verbosity Level for the session logs. Default = 0. Valid values are >=0.
            This takes into effect only when the LogSeverityLevel is set to ORT_LOGGING_LEVEL_VERBOSE.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.LogId">
            <summary>
            Log tag to be used during the run. default = ""
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.RunOptions.Terminate">
            <summary>
            Sets a flag to terminate all Run() calls that are currently using this RunOptions object 
            Default = false
            </summary>
            <value>terminate flag value</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.RunOptions.AddRunConfigEntry(System.String,System.String)">
            <summary>
            Set a single run configuration entry as a pair of strings
            If a configuration with same key exists, this will overwrite the configuration with the given configValue.
            </summary>
            <param name="configKey">config key name</param>
            <param name="configValue">config key value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.RunOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of RunOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.GraphOptimizationLevel">
            <summary>
            Graph optimization level to use with SessionOptions
             [https://github.com/microsoft/onnxruntime/blob/main/docs/ONNX_Runtime_Graph_Optimizations.md]
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.ExecutionMode">
            <summary>
            Controls whether you want to execute operators in the graph sequentially or in parallel.
            Usually when the model has many branches, setting this option to ExecutionMode.ORT_PARALLEL
            will give you better performance.
            See [ONNX_Runtime_Perf_Tuning.md] for more details.
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.SessionOptions">
            <summary>
            Holds the options for creating an InferenceSession
            It forces the instantiation of the OrtEnv singleton.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.#ctor">
            <summary>
            Constructs an empty SessionOptions
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithCudaProvider(System.Int32)">
            <summary>
            A helper method to construct a SessionOptions object for CUDA execution.
            Use only if CUDA is installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId"></param>
            <returns>A SessionsOptions() object configured for execution on deviceId</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithCudaProvider(Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions)">
            <summary>
            A helper method to construct a SessionOptions object for CUDA execution provider.
            Use only if CUDA is installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="cudaProviderOptions">CUDA EP provider options</param>
            <returns>A SessionsOptions() object configured for execution on provider options</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithTensorrtProvider(System.Int32)">
            <summary>
            A helper method to construct a SessionOptions object for TensorRT execution.
            Use only if CUDA/TensorRT are installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId"></param>
            <returns>A SessionsOptions() object configured for execution on deviceId</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithTensorrtProvider(Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions)">
            <summary>
            A helper method to construct a SessionOptions object for TensorRT execution provider.
            Use only if CUDA/TensorRT are installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="trtProviderOptions">TensorRT EP provider options</param>
            <returns>A SessionsOptions() object configured for execution on provider options</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithTvmProvider(System.String)">
            <summary>
            A helper method to construct a SessionOptions object for TVM execution.
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="settings">settings string, comprises of comma separated key:value pairs. default is empty</param>
            <returns>A SessionsOptions() object configured for execution with TVM</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.MakeSessionOptionWithRocmProvider(System.Int32)">
            <summary>
            A helper method to construct a SessionOptions object for ROCM execution.
            Use only if ROCM is installed and you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">Device Id</param>
            <returns>A SessionsOptions() object configured for execution on deviceId</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CPU(System.Int32)">
            <summary>
            Appends CPU EP to a list of available execution providers for the session.
            </summary>
            <param name="useArena">1 - use arena, 0 - do not use arena</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Dnnl(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="useArena">1 - use allocation arena, 0 - otherwise</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CUDA(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">integer device ID</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CUDA(Microsoft.ML.OnnxRuntime.OrtCUDAProviderOptions)">
            <summary>
            Append a CUDA EP instance (based on specified configuration) to the SessionOptions instance.
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="cudaProviderOptions">CUDA EP provider options</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_DML(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_OpenVINO(System.String)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification, default empty string</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Tensorrt(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Tensorrt(Microsoft.ML.OnnxRuntime.OrtTensorRTProviderOptions)">
            <summary>
            Append a TensorRT EP instance (based on specified configuration) to the SessionOptions instance.
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="trtProviderOptions">TensorRT EP provider options</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_ROCM(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">Device Id</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_MIGraphX(System.Int32)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="deviceId">device identification</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Nnapi(Microsoft.ML.OnnxRuntime.NnapiFlags)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="nnapiFlags">NNAPI specific flag mask</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_CoreML(Microsoft.ML.OnnxRuntime.CoreMLFlags)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="coremlFlags">CoreML specific flags</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider_Tvm(System.String)">
            <summary>
            Use only if you have the onnxruntime package specific to this Execution Provider.
            </summary>
            <param name="settings">string with TVM specific settings</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AppendExecutionProvider(System.String,System.Collections.Generic.Dictionary{System.String,System.String})">
            <summary>
            Append QNN, SNPE or XNNPACK execution provider
            </summary>
            <param name="providerName">Execution provider to add. 'QNN', 'SNPE' or 'XNNPACK' are currently supported.</param>
            <param name="providerOptions">Optional key/value pairs to specify execution provider options.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.RegisterCustomOpLibrary(System.String)">
            <summary>
            Loads a DLL named 'libraryPath' and looks for this entry point:
              OrtStatus* RegisterCustomOps(OrtSessionOptions* options, const OrtApiBase* api);
            It then passes in the provided session options to this function along with the api base.
            
            Prior to v1.15 this leaked the library handle and RegisterCustomOpLibraryV2 
            was added to resolve that. 
            
            From v1.15 on ONNX Runtime will manage the lifetime of the handle.
            </summary>
            <param name="libraryPath">path to the custom op library</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.RegisterCustomOpLibraryV2(System.String,System.IntPtr@)">
            <summary>
            Loads a DLL named 'libraryPath' and looks for this entry point:
            OrtStatus* RegisterCustomOps(OrtSessionOptions* options, const OrtApiBase* api);
            It then passes in the provided session options to this function along with the api base.
            The handle to the loaded library is returned in 'libraryHandle'.
            It can be unloaded by the caller after all sessions using the passed in
            session options are destroyed, or if an error occurs and it is non null.
            Hint: .NET Core 3.1 has a 'NativeLibrary' class that can be used to free the library handle
            </summary>
            <param name="libraryPath">Custom op library path</param>
            <param name="libraryHandle">out parameter, library handle</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.RegisterOrtExtensions">
            <summary>
            Register the custom operators from the Microsoft.ML.OnnxRuntime.Extensions NuGet package.
            A reference to Microsoft.ML.OnnxRuntime.Extensions must be manually added to your project. 
            </summary>
            <exception cref="T:Microsoft.ML.OnnxRuntime.OnnxRuntimeException">Throws if the extensions library is not found.</exception>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddInitializer(System.String,Microsoft.ML.OnnxRuntime.OrtValue)">
            <summary>
            Add a pre-allocated initializer to a session. If a model contains an initializer with a name
            that is same as the name passed to this API call, ORT will use this initializer instance
            instead of deserializing one from the model file. This is useful when you want to share
            the same initializer across sessions.
            </summary>
            <param name="name">name of the initializer</param>
            <param name="ortValue">OrtValue containing the initializer. Lifetime of 'val' and the underlying initializer buffer must be
            managed by the user (created using the CreateTensorWithDataAsOrtValue API) and it must outlive the session object</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddSessionConfigEntry(System.String,System.String)">
            <summary>
            Set a single session configuration entry as a pair of strings
            If a configuration with same key exists, this will overwrite the configuration with the given configValue
            </summary>
            <param name="configKey">config key name</param>
            <param name="configValue">config key value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddFreeDimensionOverride(System.String,System.Int64)">
            <summary>
            Override symbolic dimensions (by specific denotation strings) with actual values if known at session initialization time to enable
            optimizations that can take advantage of fixed values (such as memory planning, etc)
            </summary>
            <param name="dimDenotation">denotation name</param>
            <param name="dimValue">denotation value</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.AddFreeDimensionOverrideByName(System.String,System.Int64)">
            <summary>
            Override symbolic dimensions (by specific name strings) with actual values if known at session initialization time to enable
            optimizations that can take advantage of fixed values (such as memory planning, etc)
            </summary>
            <param name="dimName">dimension name</param>
            <param name="dimValue">dimension value</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.EnableMemoryPattern">
            <summary>
            Enables the use of the memory allocation patterns in the first Run() call for subsequent runs. Default = true.
            </summary>
            <value>returns enableMemoryPattern flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.ProfileOutputPathPrefix">
            <summary>
            Path prefix to use for output of profiling data
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.EnableProfiling">
            <summary>
            Enables profiling of InferenceSession.Run() calls. Default is false
            </summary>
            <value>returns _enableProfiling flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.OptimizedModelFilePath">
            <summary>
             Set filepath to save optimized model after graph level transformations. Default is empty, which implies saving is disabled.
            </summary>
            <value>returns _optimizedModelFilePath flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.EnableCpuMemArena">
            <summary>
            Enables Arena allocator for the CPU memory allocations. Default is true.
            </summary>
            <value>returns _enableCpuMemArena flag value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.LogId">
            <summary>
            Log Id to be used for the session. Default is empty string.
            </summary>
            <value>returns _logId value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.LogSeverityLevel">
            <summary>
            Log Severity Level for the session logs. Default = ORT_LOGGING_LEVEL_WARNING
            </summary>
            <value>returns _logSeverityLevel value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.LogVerbosityLevel">
            <summary>
            Log Verbosity Level for the session logs. Default = 0. Valid values are >=0.
            This takes into effect only when the LogSeverityLevel is set to ORT_LOGGING_LEVEL_VERBOSE.
            </summary>
            <value>returns _logVerbosityLevel value</value>
        </member>
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.SessionOptions.IntraOpNumThreads" -->
        <!-- Badly formed XML comment ignored for member "P:Microsoft.ML.OnnxRuntime.SessionOptions.InterOpNumThreads" -->
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.GraphOptimizationLevel">
            <summary>
            Sets the graph optimization level for the session. Default is set to ORT_ENABLE_ALL.
            </summary>
            <value>returns _graphOptimizationLevel value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.SessionOptions.ExecutionMode">
            <summary>
            Sets the execution mode for the session. Default is set to ORT_SEQUENTIAL.
            See [ONNX_Runtime_Perf_Tuning.md] for more details.
            </summary>
            <value>returns _executionMode value</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptions.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of SessionOptions
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.SessionOptionsContainer">
            <summary>
            Helper to allow the creation/addition of session options based on pre-defined named entries.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Register(System.Action{Microsoft.ML.OnnxRuntime.SessionOptions})">
            <summary>
            Register the default handler. This is used when a configuration name is not provided.
            </summary>
            <param name="defaultHandler">Handler that applies the default settings to a SessionOptions instance.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Register(System.String,System.Action{Microsoft.ML.OnnxRuntime.SessionOptions})">
            <summary>
            Register a named handler.
            </summary>
            <param name="configuration">Configuration name.</param>
            <param name="handler">
            Handler that applies the settings for the configuration to a SessionOptions instance.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Create(System.String,System.Boolean)">
            <summary>
            Create a SessionOptions instance with configuration applied.
            </summary>
            <param name="configuration">
            Configuration to use. 
            If not provided, the default set of session options will be applied if useDefaultAsFallback is true.
            </param>
            <param name="useDefaultAsFallback">
            If configuration is not provided or not found, use the default session options.
            </param>
            <returns>SessionOptions with configuration applied.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.Reset">
            <summary>
            Reset by removing all registered handlers.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.SessionOptionsContainer.ApplyConfiguration(Microsoft.ML.OnnxRuntime.SessionOptions,System.String,System.Boolean)">
            <summary>
            Apply a configuration to a SessionOptions instance.
            </summary>
            <param name="options">SessionOptions to apply configuration to.</param>
            <param name="configuration">Configuration to use.</param>
            <param name="useDefaultAsFallback">
            Use the default configuration if 'configuration' is not provided or not found.
            </param>
            <returns>Updated SessionOptions instance.</returns>
        </member>
        <!-- Badly formed XML comment ignored for member "T:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions" -->
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[])">
            <summary>
            Creates a copy of this single-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <returns>A 1-dimensional DenseTensor&lt;T&gt; with the same length and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[0:,0:],System.Boolean)">
            <summary>
            Creates a copy of this two-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): row-major.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): column-major.</param>
            <returns>A 2-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[0:,0:,0:],System.Boolean)">
            <summary>
            Creates a copy of this three-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
            <returns>A 3-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(``0[0:,0:,0:,0:],System.Boolean)">
            <summary>
            Creates a copy of this four-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
            <returns>A 4-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayTensorExtensions.ToTensor``1(System.Array,System.Boolean)">
            <summary>
            Creates a copy of this n-dimensional array as a DenseTensor&lt;T&gt;
            </summary>
            <typeparam name="T">Type contained in the array to copy to the DenseTensor&lt;T&gt;.</typeparam>
            <param name="array">The array to create a DenseTensor&lt;T&gt; from.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
            <returns>A n-dimensional DenseTensor&lt;T&gt; with the same dimensions and content as <paramref name="array"/>.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetStrides(System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Gets the set of strides that can be used to calculate the offset of n-dimensions in a 1-dimensional layout
            </summary>
            <param name="dimensions"></param>
            <param name="reverseStride"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetIndex(System.Int32[],System.ReadOnlySpan{System.Int32},System.Int32)">
            <summary>
            Calculates the 1-d index for n-d indices in layout specified by strides.
            </summary>
            <param name="strides"></param>
            <param name="indices"></param>
            <param name="startFromDimension"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetIndices(System.ReadOnlySpan{System.Int32},System.Boolean,System.Int32,System.Int32[],System.Int32)">
            <summary>
            Calculates the n-d indices from the 1-d index in a layout specificed by strides
            </summary>
            <param name="strides"></param>
            <param name="reverseStride"></param>
            <param name="index"></param>
            <param name="indices"></param>
            <param name="startFromDimension"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.GetIndices(System.ReadOnlySpan{System.Int32},System.Boolean,System.Int32,System.Span{System.Int32},System.Int32)">
            <summary>
            Calculates the n-d indices from the 1-d index in a layout specificed by strides
            </summary>
            <param name="strides"></param>
            <param name="reverseStride"></param>
            <param name="index"></param>
            <param name="indices"></param>
            <param name="startFromDimension"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.ArrayUtilities.TransformIndexByStrides(System.Int32,System.Int32[],System.Boolean,System.Int32[])">
            <summary>
            Takes an 1-d index over n-d sourceStrides and recalculates it assuming same n-d coordinates over a different n-d strides
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1">
            <summary>
            Represents a multi-dimensional collection of objects of type T that can be accessed by indices.  
            DenseTensor stores values in a contiguous sequential block of memory where all values are represented.
            </summary>
            <typeparam name="T">
            Type contained within the Tensor. Typically a value type such as int, double, float, etc.
            </typeparam>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.#ctor(System.Int32)">
            <summary>
            Initializes a rank-1 Tensor using the specified <paramref name="length"/>.
            </summary>
            <param name="length">Size of the 1-dimensional tensor</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.#ctor(System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Initializes a rank-n Tensor using the dimensions specified in <paramref name="dimensions"/>.
            </summary>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.
            </param>
            <param name="reverseStride">
            False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension 
            is most minor (closest together): akin to row-major in a rank-2 tensor.  
            True to indicate that the last dimension is most major (farthest apart) and the first dimension is most 
            minor (closest together): akin to column-major in a rank-2 tensor.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.#ctor(System.Memory{`0},System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Constructs a new DenseTensor of the specified dimensions, wrapping existing backing memory for the contents.
            </summary>
            <param name="memory"></param>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <param name="reverseStride">
            False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension 
            is most minor (closest together): akin to row-major in a rank-2 tensor.  
            True to indicate that the last dimension is most major (farthest apart) and the first dimension is most 
            minor (closest together): akin to column-major in a rank-2 tensor.
            </param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.Buffer">
            <summary>
            Memory storing backing values of this tensor.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.GetValue(System.Int32)">
            <summary>
            Gets the value at the specified index, where index is a linearized version of n-dimension indices 
            using strides. For a scalar, use index = 0
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.SetValue(System.Int32,`0)">
            <summary>
            Sets the value at the specified index, where index is a linearized version of n-dimension indices 
            using strides. For a scalar, use index = 0
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <param name="value">The new value to set at the specified position in this Tensor.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.CopyTo(`0[],System.Int32)">
            <summary>
            Overrides Tensor.CopyTo(). Copies the content of the Tensor
            to the specified array starting with arrayIndex
            </summary>
            <param name="array">destination array</param>
            <param name="arrayIndex">start index</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.IndexOf(`0)">
            <summary>
            Determines the index of a specific item in the Tensor&lt;T&gt;.
            </summary>
            <param name="item">Object to locate</param>
            <returns>The index of item if found in the tensor; otherwise, -1</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.Clone">
            <summary>
            Creates a shallow copy of this tensor, with new backing storage.
            </summary>
            <returns>A shallow copy of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.CloneEmpty``1(System.ReadOnlySpan{System.Int32})">
            <summary>
            Creates a new Tensor of a different type with the specified dimensions and the same layout as this tensor 
            with elements initialized to their default value.
            </summary>
            <typeparam name="TResult">Type contained in the returned Tensor.</typeparam>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new tensor with the same layout as this tensor but different type and dimensions.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.DenseTensor`1.Reshape(System.ReadOnlySpan{System.Int32})">
            <summary>
            Reshapes the current tensor to new dimensions, using the same backing storage.
            </summary>
            <param name="dimensions">
            An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new tensor that reinterprets backing Buffer of this tensor with different dimensions.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorElementType">
            <summary>
            Supported Tensor DataType
            </summary>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.Float16">
            <summary>
            This value type represents A Float16 value
            it is blittable as defined in https://docs.microsoft.com/en-us/dotnet/framework/interop/blittable-and-non-blittable-types
            and as such, represented the same way in managed and native memories. This means that arrays of this type
            do not have to be copied to be passed to native memory but simply pinnned and read by native code. Thus,
            one can create a Tensor on top of an array of these structures and feed it directly to Onnxruntime library.
            Binary wise, it is the same as ushort[] (uint16_t in C++). However, we would like a separate type for type dispatching.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.Tensors.Float16.value">
            <summary>
            float16 representation bits
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.#ctor(System.UInt16)">
            <summary>
            Ctor
            </summary>
            <param name="v"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.op_Implicit(Microsoft.ML.OnnxRuntime.Tensors.Float16)~System.UInt16">
            <summary>
            Converts to ushort
            </summary>
            <param name="f">instance of Float16</param>
            <returns>value member</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.op_Implicit(System.UInt16)~Microsoft.ML.OnnxRuntime.Tensors.Float16">
            <summary>
            Converts a 16-bit unsigned integer to a Float16.
            </summary>
            <param name="value">A 16-bit unsigned integer.</param>
            <returns>A Float16 that represents the converted 16-bit unsigned integer.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.op_Equality(Microsoft.ML.OnnxRuntime.Tensors.Float16,Microsoft.ML.OnnxRuntime.Tensors.Float16)">
            <summary>
            Compares values of two Float16 for binary equality
            </summary>
            <param name="lhs"></param>
            <param name="rhs"></param>
            <returns>result of value comparisons</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.op_Inequality(Microsoft.ML.OnnxRuntime.Tensors.Float16,Microsoft.ML.OnnxRuntime.Tensors.Float16)">
            <summary>
            Compares values of two Float16 for binary inequality
            </summary>
            <param name="lhs"></param>
            <param name="rhs"></param>
            <returns>result of value comparisons</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.Equals(Microsoft.ML.OnnxRuntime.Tensors.Float16)">
            <summary>
            Returns a value indicating whether this instance and other Float16 represent the same value.
            </summary>
            <param name="other">A Float16 object to compare to this instance.</param>
            <returns>true if other.value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.Equals(System.Object)">
            <summary>
            Returns a value indicating whether this instance and a specified System.Object
            represent the same type and value.
            </summary>
            <param name="obj">An System.Object.</param>
            <returns>true if obj is Float16 and its value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Float16.GetHashCode">
            <summary>
            Returns the hash code for this instance.
            </summary>
            <returns>A 32-bit signed integer hash code.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.BFloat16">
            <summary>
            This value type represents A BFloat16 value
            it is blittable as defined in https://docs.microsoft.com/en-us/dotnet/framework/interop/blittable-and-non-blittable-types
            and as such, represented the same way in managed and native memories. This means that arrays of this type
            do not have to be copied to be passed to native memory but simply pinnned and read by native code. Thus,
            one can create a Tensor on top of an array of these structures and feed it directly to Onnxruntime library.
            Binary wise, it is the same as ushort[] (uint16_t in C++). However, we would like a separate type for type dispatching.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.value">
            <summary>
            bfloat16 representation bits
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.#ctor(System.UInt16)">
            <summary>
            Ctor
            </summary>
            <param name="v"></param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.op_Implicit(Microsoft.ML.OnnxRuntime.Tensors.BFloat16)~System.UInt16">
            <summary>
            Converts to ushort
            </summary>
            <param name="bf">instance of BFloat16</param>
            <returns>value member</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.op_Implicit(System.UInt16)~Microsoft.ML.OnnxRuntime.Tensors.BFloat16">
            <summary>
            Converts a 16-bit unsigned integer to a BFloat16.
            </summary>
            <param name="value">A 16-bit unsigned integer.</param>
            <returns>A BFloat16 that represents the converted 16-bit unsigned integer.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.op_Equality(Microsoft.ML.OnnxRuntime.Tensors.BFloat16,Microsoft.ML.OnnxRuntime.Tensors.BFloat16)">
            <summary>
            Compares values of two BFloat16 for binary equality
            </summary>
            <param name="lhs"></param>
            <param name="rhs"></param>
            <returns>result of value comparisons</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.op_Inequality(Microsoft.ML.OnnxRuntime.Tensors.BFloat16,Microsoft.ML.OnnxRuntime.Tensors.BFloat16)">
            <summary>
            Compares values of two BFloat16 for binary inequality
            </summary>
            <param name="lhs"></param>
            <param name="rhs"></param>
            <returns>result of value comparisons</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.Equals(Microsoft.ML.OnnxRuntime.Tensors.BFloat16)">
            <summary>
            Returns a value indicating whether this instance and other BFloat16 represent the same value.
            </summary>
            <param name="other">A BFloat16 object to compare to this instance.</param>
            <returns>true if other.value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.Equals(System.Object)">
            <summary>
            Returns a value indicating whether this instance and a specified System.Object
            represent the same type and value.
            </summary>
            <param name="obj">An System.Object.</param>
            <returns>true if obj is BFloat16 its value is equal to this instance; otherwise, false.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.BFloat16.GetHashCode">
            <summary>
            Returns the hash code for this instance.
            </summary>
            <returns>A 32-bit signed integer hash code.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo">
            <summary>
            Helps typecasting. Holds Tensor element type traits.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.ElementType">
            <summary>
            TensorElementType enum
            </summary>
            <value>type enum value</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.TypeSize">
            <summary>
            Size of the stored primitive type in bytes
            </summary>
            <value>size in bytes</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.IsString">
            <summary>
            Is the type is a string
            </summary>
            <value>true if Tensor element type is a string</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorTypeInfo.#ctor(Microsoft.ML.OnnxRuntime.Tensors.TensorElementType,System.Int32)">
            <summary>
            Ctor
            </summary>
            <param name="elementType">TensorElementType value</param>
            <param name="typeSize">size fo the type in bytes</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo">
            <summary>
            Holds TensorElement traits
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.TensorType">
            <summary>
            Tensor element type
            </summary>
            <value>System.Type</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.TypeSize">
            <summary>
            Size of the stored primitive type in bytes
            </summary>
            <value>size in bytes</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.IsString">
            <summary>
            Is the type is a string
            </summary>
            <value>true if Tensor element type is a string</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorElementTypeInfo.#ctor(System.Type,System.Int32)">
            <summary>
            Ctor
            </summary>
            <param name="type">Tensor element type</param>
            <param name="typeSize">typesize</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.TensorBase">
            <summary>
            This class is a base for all Tensors. It hosts maps with type traits.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.#ctor(System.Type)">
            <summary>
            Constructs TensorBae
            </summary>
            <param name="primitiveType">primitive type the deriving class is using</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.GetTypeInfo(System.Type)">
            <summary>
            Query TensorTypeInfo by one of the supported types
            </summary>
            <param name="type"></param>
            <returns>TensorTypeInfo or null if not supported</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.GetElementTypeInfo(Microsoft.ML.OnnxRuntime.Tensors.TensorElementType)">
            <summary>
            Query TensorElementTypeInfo by enum
            </summary>
            <param name="elementType">type enum</param>
            <returns>instance of TensorElementTypeInfo or null if not found</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.TensorBase.GetTypeInfo">
            <summary>
            Query TensorTypeInfo using this Tensor type
            </summary>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.Tensor">
            <summary>
            Various methods for creating and manipulating Tensor&lt;T&gt;
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateIdentity``1(System.Int32)">
            <summary>
            Creates an identity tensor of the specified size.  An identity tensor is a two dimensional tensor with 1s in the diagonal.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="size">Width and height of the identity tensor to create.</param>
            <returns>a <paramref name="size"/> by <paramref name="size"/> with 1s along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateIdentity``1(System.Int32,System.Boolean)">
            <summary>
            Creates an identity tensor of the specified size and layout (row vs column major).  An identity tensor is a two dimensional tensor with 1s in the diagonal.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="size">Width and height of the identity tensor to create.</param>
            <param name="columMajor">>False to indicate that the first dimension is most minor (closest) and the last dimension is most major (farthest): row-major.  True to indicate that the last dimension is most minor (closest together) and the first dimension is most major (farthest apart): column-major.</param>
            <returns>a <paramref name="size"/> by <paramref name="size"/> with 1s along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateIdentity``1(System.Int32,System.Boolean,``0)">
            <summary>
            Creates an identity tensor of the specified size and layout (row vs column major) using the specified one value.  An identity tensor is a two dimensional tensor with 1s in the diagonal.  This may be used in case T is a type that doesn't have a known 1 value.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="size">Width and height of the identity tensor to create.</param>
            <param name="columMajor">>False to indicate that the first dimension is most minor (closest) and the last dimension is most major (farthest): row-major.  True to indicate that the last dimension is most minor (closest together) and the first dimension is most major (farthest apart): column-major.</param>
            <param name="oneValue">Value of <typeparamref name="T"/> that is used along the diagonal.</param>
            <returns>a <paramref name="size"/> by <paramref name="size"/> with 1s along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateFromDiagonal``1(Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0})">
            <summary>
            Creates a n+1-rank tensor using the specified n-rank diagonal.  Values not on the diagonal will be filled with zeros.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="diagonal">Tensor representing the diagonal to build the new tensor from.</param>
            <returns>A new tensor of the same layout and order as <paramref name="diagonal"/> of one higher rank, with the values of <paramref name="diagonal"/> along the diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor.CreateFromDiagonal``1(Microsoft.ML.OnnxRuntime.Tensors.Tensor{``0},System.Int32)">
            <summary>
            Creates a n+1-dimension tensor using the specified n-dimension diagonal at the specified offset 
            from the center.  Values not on the diagonal will be filled with zeros.
            </summary>
            <typeparam name="T">
            type contained within the Tensor. Typically a value type such as int, double, float, etc.</typeparam>
            <param name="diagonal">Tensor representing the diagonal to build the new tensor from.</param>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, 
            less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>A new tensor of the same layout and order as <paramref name="diagonal"/> of one higher rank, 
            with the values of <paramref name="diagonal"/> along the specified diagonal and zeros elsewhere.</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1">
            <summary>
            Represents a multi-dimensional collection of objects of type T that can be accessed by indices.
            </summary>
            <typeparam name="T">type contained within the Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.#ctor(System.Int32)">
            <summary>
            Initialize a 1-dimensional tensor of the specified length
            </summary>
            <param name="length">Size of the 1-dimensional tensor</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.#ctor(System.ReadOnlySpan{System.Int32},System.Boolean)">
            <summary>
            Initialize an n-dimensional tensor with the specified dimensions and layout.  ReverseStride=true gives a stride of 1-element width to the first dimension (0).  ReverseStride=false gives a stride of 1-element width to the last dimension (n-1).
            </summary>
            <param name="dimensions">An span of integers that represent the size of each dimension of the Tensor to create.</param>
            <param name="reverseStride">False (default) to indicate that the first dimension is most major (farthest apart) and the last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  True to indicate that the last dimension is most major (farthest apart) and the first dimension is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.#ctor(System.Array,System.Boolean)">
            <summary>
            Initializes tensor with same dimensions as array, content of array is ignored.  
            ReverseStride=true gives a stride of 1-element width to the first dimension (0).  
            ReverseStride=false gives a stride of 1-element width to the last dimension (n-1).
            </summary>
            <param name="fromArray">Array from which to derive dimensions.</param>
            <param name="reverseStride">
            False (default) to indicate that the first dimension is most major (farthest apart) and the 
            last dimension is most minor (closest together): akin to row-major in a rank-2 tensor.  
            True to indicate that the last dimension is most major (farthest apart) and the first dimension 
            is most minor (closest together): akin to column-major in a rank-2 tensor.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Length">
            <summary>
            Total length of the Tensor.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Rank">
            <summary>
            Rank of the tensor: number of dimensions.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IsReversedStride">
            <summary>
            True if strides are reversed (AKA Column-major)
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Dimensions">
            <summary>
            Returns a readonly view of the dimensions of this tensor.
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Strides">
            <summary>
            Returns a readonly view of the strides of this tensor.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Fill(`0)">
            <summary>
            Sets all elements in Tensor to <paramref name="value"/>.
            </summary>
            <param name="value">Value to fill</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Clone">
            <summary>
            Creates a shallow copy of this tensor, with new backing storage.
            </summary>
            <returns>A shallow copy of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty">
            <summary>
            Creates a new Tensor with the same layout and dimensions as this tensor with elements initialized to their default value.
            </summary>
            <returns>A new Tensor with the same layout and dimensions as this tensor with elements initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty(System.ReadOnlySpan{System.Int32})">
            <summary>
            Creates a new Tensor with the specified dimensions and the same layout as this tensor with elements initialized to their default value.
            </summary>
            <param name="dimensions">An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new Tensor with the same layout as this tensor and specified <paramref name="dimensions"/> with elements initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty``1">
            <summary>
            Creates a new Tensor of a different type with the same layout and size as this tensor with elements initialized to their default value.
            </summary>
            <typeparam name="TResult">Type contained within the new Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <returns>A new Tensor with the same layout and dimensions as this tensor with elements of <typeparamref name="TResult"/> type initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CloneEmpty``1(System.ReadOnlySpan{System.Int32})">
            <summary>
            Creates a new Tensor of a different type with the specified dimensions and the same layout as this tensor with elements initialized to their default value.
            </summary>
            <typeparam name="TResult">Type contained within the new Tensor.  Typically a value type such as int, double, float, etc.</typeparam>
            <param name="dimensions">An span of integers that represent the size of each dimension of the DenseTensor to create.</param>
            <returns>A new Tensor with the same layout as this tensor of specified <paramref name="dimensions"/> with elements of <typeparamref name="TResult"/> type initialized to their default value.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetDiagonal">
            <summary>
            Gets the n-1 dimension diagonal from the n dimension tensor.
            </summary>
            <returns>An n-1 dimension tensor with the values from the main diagonal of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetDiagonal(System.Int32)">
            <summary>
            Gets the n-1 dimension diagonal from the n dimension tensor at the specified offset from center.
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>An n-1 dimension tensor with the values from the specified diagonal of this tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetTriangle">
            <summary>
            Gets a tensor representing the elements below and including the diagonal, with the rest of the elements zero-ed.
            </summary>
            <returns>A tensor with the values from this tensor at and below the main diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetTriangle(System.Int32)">
            <summary>
            Gets a tensor representing the elements below and including the specified diagonal, with the rest of the elements zero-ed.
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>A tensor with the values from this tensor at and below the specified diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetUpperTriangle">
            <summary>
            Gets a tensor representing the elements above and including the diagonal, with the rest of the elements zero-ed.
            </summary>
            <returns>A tensor with the values from this tensor at and above the main diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetUpperTriangle(System.Int32)">
            <summary>
            Gets a tensor representing the elements above and including the specified diagonal, with the rest of the elements zero-ed.
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.  0 for the main diagonal, less than zero for diagonals below, greater than zero from diagonals above.</param>
            <returns>A tensor with the values from this tensor at and above the specified diagonal and zeros elsewhere.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetTriangle(System.Int32,System.Boolean)">
            <summary>
            Implementation method for GetTriangle, GetLowerTriangle, GetUpperTriangle
            </summary>
            <param name="offset">Offset of diagonal to set in returned tensor.</param>
            <param name="upper">true for upper triangular and false otherwise</param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Reshape(System.ReadOnlySpan{System.Int32})">
            <summary>
            Reshapes the current tensor to new dimensions, using the same backing storage if possible.
            </summary>
            <param name="dimensions">An span of integers that represent the size of each dimension of the Tensor to create.</param>
            <returns>A new tensor that reinterprets this tensor with different dimensions.</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Item(System.Int32[])">
            <summary>
            Obtains the value at the specified indices
            </summary>
            <param name="indices">A one-dimensional array of integers that represent the indices specifying the position of the element to get.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Item(System.ReadOnlySpan{System.Int32})">
            <summary>
            Obtains the value at the specified indices
            </summary>
            <param name="indices">A span integers that represent the indices specifying the position of the element to get.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetValue(System.Int32)">
            <summary>
            Gets the value at the specied index, where index is a linearized version of n-dimension indices using strides.
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <returns>The value at the specified position in this Tensor.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.SetValue(System.Int32,`0)">
            <summary>
            Sets the value at the specied index, where index is a linearized version of n-dimension indices using strides.
            </summary>
            <param name="index">An integer index computed as a dot-product of indices.</param>
            <param name="value">The new value to set at the specified position in this Tensor.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Compare(Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0},Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0})">
            <summary>
            Performs a value comparison of the content and shape of two tensors.  Two tensors are equal if they have the same shape and same value at every set of indices.  If not equal a tensor is greater or less than another tensor based on the first non-equal element when enumerating in linear order.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Equals(Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0},Microsoft.ML.OnnxRuntime.Tensors.Tensor{`0})">
            <summary>
            Performs a value equality comparison of the content of two tensors. Two tensors are equal if they have the same shape and same value at every set of indices.
            </summary>
            <param name="left"></param>
            <param name="right"></param>
            <returns></returns>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IsFixedSize">
            <summary>
            Always fixed size Tensor
            </summary>
            <value>always true</value>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IsReadOnly">
            <summary>
            Tensor is not readonly
            </summary>
            <value>always false</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.Contains(`0)">
            <summary>
            Determines whether an element is in the Tensor&lt;T&gt;.
            </summary>
            <param name="item">
            The object to locate in the Tensor&lt;T&gt;. The value can be null for reference types.
            </param>
            <returns>
            true if item is found in the Tensor&lt;T&gt;; otherwise, false.
            </returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.CopyTo(`0[],System.Int32)">
            <summary>
            Copies the elements of the Tensor&lt;T&gt; to an Array, starting at a particular Array index.
            </summary>
            <param name="array">
            The one-dimensional Array that is the destination of the elements copied from Tensor&lt;T&gt;. The Array must have zero-based indexing.
            </param>
            <param name="arrayIndex">
            The zero-based index in array at which copying begins.
            </param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.IndexOf(`0)">
            <summary>
            Determines the index of a specific item in the Tensor&lt;T&gt;.
            </summary>
            <param name="item">The object to locate in the Tensor&lt;T&gt;.</param>
            <returns>The index of item if found in the tensor; otherwise, -1.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.ToDenseTensor">
            <summary>
            Creates a copy of this tensor as a DenseTensor&lt;T&gt;.  If this tensor is already a DenseTensor&lt;T&gt; calling this method is equivalent to calling Clone().
            </summary>
            <returns></returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.Tensors.Tensor`1.GetArrayString(System.Boolean)">
            <summary>
            Get a string representation of Tensor
            </summary>
            <param name="includeWhitespace"></param>
            <returns></returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.CheckpointState">
            <summary>
             Holds the Checkpoint State as generated/consumed by on-device training APIs
            </summary>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.CheckpointState.IsInvalid">
            <summary>
            Overrides SafeHandle.IsInvalid
            </summary>
            <value>returns true if handle is equal to Zero</value>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.LoadCheckpoint(System.String)">
            <summary>
            Loads Checkpoint state from path
            </summary>
            <param name="checkpointPath"> absolute path to checkpoint</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.SaveCheckpoint(Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.Boolean)">
            <summary>
            Saves the checkpoint
            <param name="checkpointPath"> absolute path to the checkpoint file.</param>
            <param name="includeOptimizerState"> absolute path to the checkpoint file.</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.AddProperty(System.String,System.Int64)">
            <summary>
            Adds the given int property to the checkpoint state.
            <param name="propertyName">Unique name of the property being added.</param>
            <param name="propertyValue">Property value associated with the given name.</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.AddProperty(System.String,System.Single)">
            <summary>
            Adds the given float property to the checkpoint state.
            <param name="propertyName">Unique name of the property being added.</param>
            <param name="propertyValue">Property value associated with the given name.</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.AddProperty(System.String,System.String)">
            <summary>
            Adds the given string property to the checkpoint state.
            <param name="propertyName">Unique name of the property being added.</param>
            <param name="propertyValue">Property value associated with the given name.</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.GetProperty(System.String)">
            <summary>
            Gets the property value associated with the given name from the checkpoint state.
            <param name="propertyName">Unique name of the property being retrieved.</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.CheckpointState.ReleaseHandle">
            <summary>
            Overrides SafeHandle.ReleaseHandle() to properly dispose of
            the native instance of CheckpointState
            </summary>
            <returns>always returns true</returns>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeTrainingMethods.DOrtLoadCheckpoint">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="checkpointPath">checkpoint string path</param>
            <param name="checkpointState">(Output) Loaded OrtCheckpointState instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeTrainingMethods.DOrtSaveCheckpoint">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="checkpointState">OrtCheckpointState instance to save</param>
            <param name="checkpointPath">Checkpoint string path</param>
            <param name="includeOptimizerState">Flag indicating whether to save the optimizer state.</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.NativeTrainingMethods.DOrtCreateTrainingSession">
            <summary>
            Creates an instance of OrtSession with provided parameters
            </summary>
            <param name="environment">Native OrtEnv instance</param>
            <param name="sessionOptions">Native SessionOptions instance</param>
            <param name="checkpointState">Loaded OrtCheckpointState instance</param>
            <param name="trainModelPath">model string path</param>
            <param name="evalModelPath">model string path</param>
            <param name="optimizerModelPath">model string path</param>
            <param name="session">(Output) Created native OrtTrainingSession instance</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.TrainingUtils">
            <summary>
            This class defines utility methods for training.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingUtils.SetSeed(System.Int64)">
            <summary>
            Use this function to generate reproducible results. It should be noted that completely
            reproducible results are not guaranteed.
            </summary>
            <param name="seed">Manual seed to use for random number generation.</param>
        </member>
        <member name="T:Microsoft.ML.OnnxRuntime.TrainingSession">
            <summary>
            Represents a Training Session on an ONNX Model.
            This is a IDisposable class and it must be disposed of
            using either a explicit call to Dispose() method or
            a pattern of using() block. If this is a member of another
            class that class must also become IDisposable and it must
            dispose of TrainingSession in its Dispose() method.
            </summary>
        </member>
        <member name="F:Microsoft.ML.OnnxRuntime.TrainingSession._nativeHandle">
            <summary>
            A pointer to a underlying native instance of OrtTrainingSession
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.#ctor(Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.String,System.String)">
            <summary>
            Creates TrainingSession from the model and checkpoint in <paramref name="state"/>.
            </summary>
            <param name="state">Model checkpoint loaded into <see cref="T:Microsoft.ML.OnnxRuntime.CheckpointState"/>.</param>
            <param name="trainModelPath">Specify path to training model graph.</param>
            <param name="evalModelPath">Specify path to eval model graph.</param>
            <param name="optimizerModelPath">Specify path to optimizer model graph.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.#ctor(Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.String)">
            <summary>
            Creates TrainingSession from the model and checkpoint in <paramref name="state"/>.
            </summary>
            <param name="state">Model checkpoint loaded into <see cref="T:Microsoft.ML.OnnxRuntime.CheckpointState"/>.</param>
            <param name="trainModelPath">Specify path to training model graph.</param>
            <param name="optimizerModelPath">Specify path to optimizer model graph.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.#ctor(Microsoft.ML.OnnxRuntime.CheckpointState,System.String)">
            <summary>
            Creates TrainingSession from the model and checkpoint in <paramref name="state"/>.
            </summary>
            <param name="state">Model checkpoint loaded into <see cref="T:Microsoft.ML.OnnxRuntime.CheckpointState"/>.</param>
            <param name="trainModelPath">Specify path to training model graph.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.#ctor(Microsoft.ML.OnnxRuntime.SessionOptions,Microsoft.ML.OnnxRuntime.CheckpointState,System.String,System.String,System.String)">
            <summary>
            Creates TrainingSession from the model and checkpoint in <paramref name="state"/>.
            </summary>
            <param name="options">Session options</param>
            <param name="state">Model checkpoint loaded into <see cref="T:Microsoft.ML.OnnxRuntime.CheckpointState"/>.</param>
            <param name="trainModelPath">Specify path to training model graph.</param>
            <param name="evalModelPath">Specify path to eval model graph.</param>
            <param name="optimizerModelPath">Specify path to optimizer model graph.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs a train step on the loaded model for the given inputs.
            </summary>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs a train step on the loaded model for the given inputs. Uses the given RunOptions for this run.
            </summary>
            <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs, and fetches the graph outputs.
            </summary>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.TrainStep(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs the loaded model for the given inputs, and fetches the specified outputs in <paramref name="outputNames"/>. Uses the given RunOptions for this run.
            </summary>
            <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <returns>Output Tensors in a Collection of NamedOnnxValue. User must dispose the output.</returns>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.LazyResetGrad">
            <summary>
            Sets the reset grad flag on the training graph. The gradient buffers will be reset while executing the
            next train step.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.EvalStep(System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs an eval step on the loaded model for the given inputs. The eval graph must be passed while TrainingSession creation.
            </summary>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.EvalStep(Microsoft.ML.OnnxRuntime.RunOptions,System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue},System.Collections.Generic.IReadOnlyCollection{Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue})">
            <summary>
            Runs an eval step on the loaded model for the given inputs. The eval graph must be passed while TrainingSession creation.
            </summary>
            <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
            <param name="inputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the input values.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.SetLearningRate(System.Single)">
            <summary>
            Sets a constant learning rate for the session. LR must be controlled by either this method
            or by registering a LR scheduler.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.GetLearningRate">
            <summary>
            Gets the current learning rate for the session.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.RegisterLinearLRScheduler(System.Int64,System.Int64,System.Single)">
            <summary>
            Registers a linear learning rate scheduler for the session. LR must be controlled by either
            the SetLearningRate method or by registering a LR scheduler.
            <param name="warmupStepCount"> Number of warmup steps</param>
            <param name="totalStepCount"> Number of total steps</param>
            <param name="initialLearningRate"> Initial learning rate</param>
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.SchedulerStep">
            <summary>
            Runs a LR scheduler step. There must be a valid LR scheduler registered for the training session.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.OptimizerStep">
            <summary>
            Runs an optimizer step on the loaded model for the given inputs. The optimizer graph must be passed while TrainingSession creation.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.OptimizerStep(Microsoft.ML.OnnxRuntime.RunOptions)">
            <summary>
            Runs an eval step on the loaded model for the given inputs. The eval graph must be passed while TrainingSession creation.
            </summary>
            <param name="options">Specify <see cref="T:Microsoft.ML.OnnxRuntime.RunOptions"/> for step.</param>
            <param name="outputValues">Specify a collection of <see cref="T:Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue"/> that indicates the output values.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.ExportModelForInferencing(System.String,System.Collections.Generic.IReadOnlyCollection{System.String})">
            <summary>
            Export a model that can be used for inferencing.
            If the training session was provided with an eval model, the training session can generate
            an inference model if it knows the inference graph outputs. The input inference graph outputs
            are used to prune the eval model so that the inference model's outputs align with the provided outputs.
            The exported model is saved at the path provided and can be used for inferencing with Ort::Session.
            Note that the function re-loads the eval model from the path provided to Ort::TrainingSession
            and expects that this path still be valid.
            </summary>
            <param name="inference_model_path">Path where the inference model should be serialized to.</param>
            <param name="graphOutputNames">Names of the outputs that are needed in the inference model.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.ToBuffer(System.Boolean)">
            <summary>
            Returns a contiguous buffer that holds a copy of all training state parameters
            </summary>
            <param name="only_trainable">Whether to only copy trainable parameters or to copy all parameters.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.FromBuffer(Microsoft.ML.OnnxRuntime.FixedBufferOnnxValue)">
            <summary>
            Loads the training session model parameters from a contiguous buffer
            </summary>
            <param name="buffer">Contiguous buffer to load the parameters from.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.OutputNames(System.Boolean)">
            <summary>
            Retrieves the names of the user outputs for the training and eval models.
            </summary>
            <param name="training">Whether the training model output names are requested or eval model output names.</param>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.InputNames(System.Boolean)">
            <summary>
            Retrieves the names of the user inputs for the training and eval models.
            </summary>
            <param name="training">Whether the training model input names are requested or eval model input names.</param>
        </member>
        <member name="P:Microsoft.ML.OnnxRuntime.TrainingSession.Handle">
            <summary>
            Other classes access
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.Finalize">
            <summary>
            Finalizer.
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.Dispose">
            <summary>
            IDisposable implementation
            </summary>
        </member>
        <member name="M:Microsoft.ML.OnnxRuntime.TrainingSession.Dispose(System.Boolean)">
            <summary>
            IDisposable implementation
            </summary>
            <param name="disposing">true if invoked from Dispose() method</param>
        </member>
    </members>
</doc>
